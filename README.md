# SANTEK = Structured Artificial Intelligence Technology Kernel
### SanTOK - Advanced Text Tokenization & Cognitive Processing Framework

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/chavalasantosh/SanTOK)

**SanTOK** is a comprehensive, production-ready text processing and cognitive reasoning framework that goes far beyond simple tokenization. It provides a complete toolkit for text analysis, semantic understanding, model training, vector storage, API deployment, and deterministic reasoning.

---

## ğŸ“‹ Table of Contents

- [Quick Start (5 Minutes)](#quick-start-5-minutes)
- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Model Learning & Development Process](#model-learning--development-process---complete-workflow)
- [How SanTOK Components Help Your Model](#how-santok-components-help-your-model---practical-benefits)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Core Components](#core-components)
- [Usage Examples](#usage-examples)
- [Advanced Examples & Use Cases](#advanced-examples--use-cases)
- [API Documentation](#api-documentation)
- [CLI Usage](#cli-usage)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [Testing](#testing)
- [Comparison with Alternatives](#comparison-with-alternatives)
- [Known Limitations](#known-limitations)
- [Best Practices & Recommendations](#best-practices--recommendations)
- [Roadmap & Future Plans](#roadmap--future-plans)
- [Version History](#version-history)
- [Migration Guide](#migration-guide)
- [Quick Reference / Cheat Sheet](#quick-reference--cheat-sheet)
- [Industry Use Cases & Applications](#industry-use-cases--applications)
- [FAQ](#frequently-asked-questions-faq)
- [Contributing](#contributing)
- [License](#license)
- [Author](#author)
- [Additional Resources](#additional-resources)
- [Support & Troubleshooting](#support--troubleshooting)

---

## âš¡ Quick Start (5 Minutes)

**Get SanTOK running in 5 minutes:**

```bash
# 1. Install
pip install -r requirements.txt

# 2. Basic tokenization
python -c "from santok import TextTokenizationEngine; engine = TextTokenizationEngine(); print(engine.tokenize('Hello World', 'word'))"

# 3. Start API server
python run.py

# 4. Test API
curl http://localhost:8000/api/v1/health
```

**Or use the CLI:**
```bash
python santok_cli.py tokenize "Hello World" --method word
```

**For detailed setup, see [Installation](#installation) section.**

---

## ğŸ¯ Overview

SanTOK is a multi-layered framework consisting of three main components:

1. **SanTOK Core** - Advanced text tokenization engine with multiple methods, mathematical analysis, and statistical features
2. **SanTOK Cognitive** - Deterministic reasoning substrate for LLM-based systems with knowledge graphs and symbolic reasoning
3. **SanTOK Complete** - Comprehensive production system with embeddings, vector stores, training, and API servers

### What Makes SanTOK Unique?

- **Multiple Tokenization Methods**: 9+ tokenization strategies (word, character, subword, grammar-based, byte-level, etc.)
- **Deterministic Processing**: Same input always produces the same output with reproducible UIDs
- **Mathematical Analysis**: Advanced algorithms using digital roots, weighted sums, and 9-centric mathematics
- **Semantic Embeddings**: Multiple embedding strategies (feature-based, hash-based, semantic, hybrid)
- **Vector Database Integration**: Support for ChromaDB, FAISS, and Weaviate
- **Cognitive Reasoning**: Knowledge graphs, symbolic reasoning, and constraint enforcement
- **Production-Ready APIs**: FastAPI-based servers with WebSocket support
- **Training Capabilities**: Custom semantic model training on your data

---

## âœ¨ Key Features

### Core Tokenization
- âœ… **9+ Tokenization Methods**: Space, word, character, grammar, subword (BPE, frequency, syllable), byte-level
- âœ… **Mathematical Properties**: Frontend digits, backend numbers, global IDs, digital roots
- âœ… **Deterministic UIDs**: Xorshift64* based unique identifiers
- âœ… **Statistical Features**: Length factors, balance indices, entropy calculations
- âœ… **Preprocessing Options**: Case normalization, punctuation removal, repetition collapsing

### Semantic Processing
- âœ… **Multiple Embedding Strategies**: Feature-based, hash-based, semantic, hybrid
- âœ… **Semantic Training**: Train custom embeddings on your corpus
- âœ… **Enhanced Trainer**: Multi-stream hierarchical learning with temporal awareness
- âœ… **Vector Stores**: ChromaDB, FAISS, Weaviate integration
- âœ… **Inference Pipeline**: Production-ready embedding inference

### Cognitive Reasoning (SanTOK Cognitive)
- âœ… **Knowledge Graphs**: 15+ relation types (IS_A, PART_OF, CAUSES, USES, etc.)
- âœ… **Symbolic Reasoning**: 20+ inference rules (transitivity, inheritance, symmetry)
- âœ… **Knowledge Trees**: Hierarchical organization and taxonomies
- âœ… **Unified Memory**: Persistent memory system with graph linking
- âœ… **Constraint Enforcement**: LLM output validation against verified facts
- âœ… **Full Explainability**: Complete reasoning traces for every answer

### API & Deployment
- âœ… **FastAPI Servers**: Production-ready RESTful APIs
- âœ… **WebSocket Support**: Real-time tokenization and streaming
- âœ… **Interactive Documentation**: Auto-generated API docs at `/docs`
- âœ… **Job Management**: Async job processing with status tracking
- âœ… **Authentication**: JWT-based security
- âœ… **Cross-Platform**: Windows, Linux, macOS support

### Training & Models
- âœ… **Vocabulary Building**: Custom vocabulary construction
- âœ… **Language Model Training**: Train language models on your data
- âœ… **Small Language Models (SLM)**: Lightweight transformer-based models
- âœ… **Dataset Management**: Download and process training datasets

---

## ğŸ—ï¸ Architecture

SanTOK follows a modular architecture with clear separation of concerns. Below is a detailed breakdown of each component's architecture.

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SanTOK Framework                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ SanTOK Core  â”‚  â”‚ SanTOK       â”‚  â”‚ SanTOK       â”‚      â”‚
â”‚  â”‚              â”‚  â”‚ Cognitive    â”‚  â”‚ Complete     â”‚      â”‚
â”‚  â”‚ Tokenization â”‚  â”‚ Reasoning    â”‚  â”‚ Production   â”‚      â”‚
â”‚  â”‚ Engine       â”‚  â”‚ Substrate    â”‚  â”‚ System       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚      API Servers & CLI Tools         â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚   Vector Stores & Integrations       â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ Clean Architecture - Component Deep Dives

This section provides a clean, detailed view of each major component's architecture, showing how they work internally and how they integrate with the rest of the system.

---

## ğŸ”¤ Tokenization Architecture - Clean Face

### Overview

SanTOK's tokenization system is the foundation of all text processing. It provides 9 different tokenization methods, each producing deterministic, mathematically-rich token representations.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Tokenization Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Input Layer                             â”‚    â”‚
â”‚  â”‚  - Raw text string                                   â”‚    â”‚
â”‚  â”‚  - Optional: source tag, language hint               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Preprocessing Pipeline                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Normalize    â”‚â†’ â”‚ Remove       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Case         â”‚  â”‚ Punctuation  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Normalize    â”‚â†’ â”‚ Detect       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Whitespace   â”‚  â”‚ Language     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Parallel Tokenization (9 Methods)              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚  Space   â”‚ â”‚  Word    â”‚ â”‚  Char    â”‚           â”‚    â”‚
â”‚  â”‚  â”‚Tokenizer â”‚ â”‚Tokenizer â”‚ â”‚Tokenizer â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Grammar  â”‚ â”‚ Subword  â”‚ â”‚  Byte    â”‚           â”‚    â”‚
â”‚  â”‚  â”‚Tokenizer â”‚ â”‚Tokenizer â”‚ â”‚Tokenizer â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚   BPE    â”‚ â”‚ Syllable â”‚ â”‚Frequency â”‚           â”‚    â”‚
â”‚  â”‚  â”‚Tokenizer â”‚ â”‚Tokenizer â”‚ â”‚Tokenizer â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Mathematical Enrichment Layer                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ UID          â”‚â†’ â”‚ Frontend     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Generation   â”‚  â”‚ Digit Calc   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚(Xorshift64*) â”‚  â”‚(9-centric)   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Backend      â”‚â†’ â”‚ Global ID    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Number       â”‚  â”‚ Assignment   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Composition  â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Neighbor     â”‚â†’ â”‚ Content ID   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ UID Linking  â”‚  â”‚ Generation   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Statistical Analysis                             â”‚    â”‚
â”‚  â”‚  - Length Factor (token count % 10)                  â”‚    â”‚
â”‚  â”‚  - Balance Index (mean of frontend digits)           â”‚    â”‚
â”‚  â”‚  - Entropy Index (variance of frontend digits)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      TokenStream Output                               â”‚    â”‚
â”‚  â”‚  - TokenRecord objects with all properties            â”‚    â”‚
â”‚  â”‚  - Organized by tokenization method                   â”‚    â”‚
â”‚  â”‚  - Ready for embedding generation                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Token Processing Flow

```
Input: "Hello World"
    â†“
[Preprocessing]
    normalize_case() â†’ "hello world"
    normalize_whitespace() â†’ "hello world"
    detect_language() â†’ "en"
    â†“
[Tokenization - Word Method]
    tokenize_word() â†’ ["Hello", "World"]
    â†“
[UID Assignment]
    assign_uids(seed=42) â†’ 
        "Hello" â†’ UID: 12345678901234567890
        "World" â†’ UID: 98765432109876543210
    â†“
[Mathematical Properties]
    Frontend Digits:
        "Hello" â†’ 5 (9-centric calculation)
        "World" â†’ 6
    Backend Numbers:
        "Hello" â†’ 12345 (composite calculation)
        "World" â†’ 67890
    Global IDs:
        "Hello" â†’ 1001
        "World" â†’ 1002
    â†“
[Neighbor Linking]
    "Hello".prev_uid = None
    "Hello".next_uid = 98765432109876543210
    "World".prev_uid = 12345678901234567890
    "World".next_uid = None
    â†“
[TokenRecord Creation]
    TokenRecord(
        text="Hello",
        uid=12345678901234567890,
        index=0,
        frontend_digit=5,
        backend_number=12345,
        global_id=1001,
        prev_uid=None,
        next_uid=98765432109876543210,
        content_id=hash("Hello")
    )
    â†“
[TokenStream]
    TokenStream(
        name="word",
        tokens=[TokenRecord("Hello"), TokenRecord("World")]
    )
```

### TokenRecord Structure

```python
TokenRecord:
    text: str                    # Original token text
    uid: int                     # Unique identifier (64-bit)
    index: int                   # Position in sequence
    frontend_digit: int          # 9-centric digit (1-9)
    backend_number: int          # Composite number
    global_id: int               # Global sequence ID
    content_id: int              # Content-based hash
    prev_uid: Optional[int]      # Previous token UID
    next_uid: Optional[int]       # Next token UID
    stream_type: str             # Tokenization method
    metadata: dict               # Additional properties
```

### Key Features

- **Deterministic**: Same input + seed = same output
- **Parallel Processing**: All 9 methods run simultaneously
- **Mathematical Richness**: Every token has 5+ mathematical properties
- **Multi-language**: Automatic language detection
- **Source Tracking**: Optional source tags for provenance

---

## ğŸ§® Embeddings Architecture - Clean Face

### Overview

SanTOK's embedding system converts tokenized text into dense vector representations suitable for machine learning, similarity search, and semantic analysis.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Embeddings Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  TokenRecord Input                                            â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Strategy Router                              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Feature  â”‚ â”‚ Semantic â”‚ â”‚   Hash   â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  Based   â”‚ â”‚ (Trained)â”‚ â”‚  Based   â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚    â”‚
â”‚  â”‚  â”‚  Hybrid  â”‚                                      â”‚    â”‚
â”‚  â”‚  â”‚(Combined)â”‚                                      â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Feature Extraction (All Strategies)            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ UID Features â”‚  â”‚ Text Featuresâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - 64-bit â†’ 8 â”‚  â”‚ - Length     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   bytes      â”‚  â”‚ - Char freq  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Math Featuresâ”‚  â”‚ Stream Featuresâ”‚               â”‚    â”‚
â”‚  â”‚  â”‚ - Frontend   â”‚  â”‚ - Type (one-hot)â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ - Backend    â”‚  â”‚ - Position     â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ - Global ID  â”‚  â”‚                â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Strategy-Specific Processing                    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  [Feature-Based]                                     â”‚    â”‚
â”‚  â”‚    Features â†’ Concatenate â†’ Project â†’ Normalize     â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  [Semantic]                                          â”‚    â”‚
â”‚  â”‚    UID â†’ Lookup in trained model â†’ Embedding         â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  [Hash-Based]                                        â”‚    â”‚
â”‚  â”‚    Text+UID â†’ Hash â†’ Vector â†’ Normalize             â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  [Hybrid]                                            â”‚    â”‚
â”‚  â”‚    Text Embedding + Feature Embedding â†’ Weighted    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Dimension Projection                            â”‚    â”‚
â”‚  â”‚  - Project to target dimension (default: 768)        â”‚    â”‚
â”‚  â”‚  - L2 normalization                                   â”‚    â”‚
â”‚  â”‚  - Type conversion (float32)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  Embedding Vector (numpy.ndarray, shape: (embedding_dim,)) â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Embedding Strategies Deep Dive

#### 1. Feature-Based Strategy

```
TokenRecord
    â†“
Extract Features:
    - UID bytes: [0.12, 0.34, 0.56, ...] (8 floats)
    - Frontend digit: [0.56] (1 float, normalized)
    - Backend number: [0.78] (1 float, normalized)
    - Global ID: [0.90] (1 float, normalized)
    - Text length: [0.05] (1 float, normalized)
    - Character frequencies: [0.1, 0.2, ...] (26 floats)
    - Stream type: [0, 1, 0, ...] (9 floats, one-hot)
    â†“
Concatenate â†’ Feature Vector (47 floats)
    â†“
Linear Projection Matrix (47 Ã— 768)
    â†“
Embedding Vector (768 floats)
    â†“
L2 Normalize
    â†“
Final Embedding
```

#### 2. Semantic Strategy

```
TokenRecord
    â†“
Extract UID: 12345678901234567890
    â†“
Lookup in Trained Model:
    vocab[uid] â†’ index: 42
    â†“
Retrieve Embedding:
    embeddings[42] â†’ [0.1, 0.2, ..., 0.9] (768 floats)
    â†“
Final Embedding (already normalized from training)
```

#### 3. Hash-Based Strategy

```
TokenRecord
    â†“
Combine: text + str(uid)
    â†“
Hash Function (SHA256)
    â†“
Convert to Vector:
    hash_bytes â†’ [0-255] â†’ normalize to [0-1]
    â†“
Repeat/Interpolate to 768 dimensions
    â†“
L2 Normalize
    â†“
Final Embedding
```

#### 4. Hybrid Strategy

```
TokenRecord
    â†“
    â”œâ”€â†’ Text Embedding (optional, from sentence-transformers)
    â”‚   â””â”€â†’ [0.1, 0.2, ..., 0.9] (768 floats)
    â”‚
    â””â”€â†’ Feature Embedding (always)
        â””â”€â†’ [0.2, 0.3, ..., 0.8] (768 floats)
    â†“
Weighted Combination:
    embedding = Î± Ã— text_emb + (1-Î±) Ã— feature_emb
    (default: Î± = 0.5)
    â†“
L2 Normalize
    â†“
Final Embedding
```

### Batch Processing

```
List[TokenRecord]
    â†“
[Parallel Processing]
    Split into chunks
    Process chunks in parallel (multiprocessing)
    â†“
[Feature Extraction]
    Extract features for all tokens
    â†“
[Vector Generation]
    Generate embeddings for all tokens
    â†“
[Stacking]
    Stack into matrix: (N, embedding_dim)
    â†“
Output: numpy.ndarray
```

---

## ğŸ§  Semantic Architecture - Clean Face

### Overview

SanTOK's semantic system learns meaningful representations from token co-occurrence patterns, context relationships, and mathematical properties without requiring pre-trained models.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Semantic System Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Training Corpus                                             â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Tokenization Phase                            â”‚    â”‚
â”‚  â”‚  - TextTokenizer.build()                             â”‚    â”‚
â”‚  â”‚  - Multiple token streams                            â”‚    â”‚
â”‚  â”‚  - TokenRecord creation                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Vocabulary Building                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Count        â”‚â†’ â”‚ Filter       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Frequencies  â”‚  â”‚ (min_count)  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Sort by      â”‚â†’ â”‚ Create       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Frequency    â”‚  â”‚ UIDâ†’Index    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Mapping      â”‚                â”‚    â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Co-occurrence Matrix Construction            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Build        â”‚â†’ â”‚ Context      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Context      â”‚  â”‚ Windows      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Windows      â”‚  â”‚ (size=5)     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Count        â”‚â†’ â”‚ Create       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Co-occurrenceâ”‚  â”‚ Sparse       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Pairs        â”‚  â”‚ Matrix       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Embedding Initialization                     â”‚    â”‚
â”‚  â”‚  - Random initialization (normal distribution)       â”‚    â”‚
â”‚  â”‚  - Token embeddings: (vocab_size, embedding_dim)     â”‚    â”‚
â”‚  â”‚  - Context embeddings: (vocab_size, embedding_dim)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Training Loop (Epochs)                       â”‚    â”‚
â”‚  â”‚  For each epoch:                                     â”‚    â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚    â”‚ Sample Training Pairs                â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Positive: co-occurring tokens      â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Negative: random tokens            â”‚          â”‚    â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â”‚                   â†“                                  â”‚    â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚    â”‚ Forward Pass                         â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Dot product: token_emb Â· ctx_emb   â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Apply sigmoid                      â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Compute loss (binary cross-entropy)â”‚          â”‚    â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â”‚                   â†“                                  â”‚    â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚    â”‚ Backward Pass                        â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Compute gradients                  â”‚          â”‚    â”‚
â”‚  â”‚    â”‚ - Update embeddings (SGD)            â”‚          â”‚    â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Model Saving                                 â”‚    â”‚
â”‚  â”‚  - Save token embeddings                             â”‚    â”‚
â”‚  â”‚  - Save vocabulary mapping                           â”‚    â”‚
â”‚  â”‚  - Save metadata (dim, vocab_size, etc.)             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Enhanced Semantic Training (Multi-Stream)

```
Multiple Token Streams
    â”œâ”€ char stream: [c, h, e, l, l, o, ...]
    â”œâ”€ subword stream: [hel, lo, wor, ld, ...]
    â””â”€ word stream: [hello, world, ...]
        â†“
[Multi-Stream Learning]
    Learn embeddings at all granularities simultaneously
    Cross-stream alignment:
        - Align char â†’ subword â†’ word
        - Hierarchical semantic relationships
    â†“
[Temporal Awareness]
    Position-dependent embeddings:
        - Early tokens: different semantics
        - Middle tokens: context-aware
        - Late tokens: summary semantics
    â†“
[Content-ID Clustering]
    Group tokens by content_id:
        - Deterministic semantic clusters
        - Similar content â†’ similar embeddings
    â†“
[Mathematical Property Integration]
    Incorporate frontend/backend/global_id:
        - Mathematical relationships â†’ semantic signals
        - UID-based semantic graph
    â†“
Enhanced Multi-Granularity Embeddings
```

### Semantic Relationships Captured

1. **Co-occurrence**: Tokens appearing together
2. **Context**: Neighbor relationships
3. **Content Similarity**: Same content_id â†’ similar meaning
4. **Temporal**: Position-dependent semantics
5. **Hierarchical**: Char â†’ Subword â†’ Word relationships
6. **Mathematical**: UID-based relationships

---

## ğŸ§  Model Learning & Development Process - Complete Workflow

### How Models Learn in SanTOK

SanTOK models learn through **self-supervised learning** using SanTOK's unique mathematical properties. The learning process is transparent, deterministic, and explainable.

---

### ğŸ“š Phase 1: Vocabulary Building (Foundation)

**Purpose**: Create a deterministic vocabulary from your text corpus using SanTOK tokenization.

**Step-by-Step Process**:

```
1. Text Corpus Input
   â””â”€ Raw text files (any size)
       â†“
2. SanTOK Tokenization
   â””â”€ Tokenize using SanTOK (word/char/subword)
   â””â”€ Extract UIDs, frontend digits, backend numbers
   â””â”€ Build TokenRecord objects
       â†“
3. Token Counting & Frequency Analysis
   â””â”€ Count occurrences of each unique token
   â””â”€ Track token metadata (UID, frontend, backend, content_id)
   â””â”€ Filter by minimum frequency threshold
       â†“
4. Vocabulary Construction
   â””â”€ Select top N tokens (default: 60,000)
   â””â”€ Assign sequential IDs to tokens
   â””â”€ Create token_to_id and id_to_token mappings
   â””â”€ Store special tokens (<PAD>, <UNK>, <BOS>, <EOS>, <MASK>)
       â†“
5. Vocabulary Persistence
   â””â”€ Save vocabulary to disk (pickle + JSON)
   â””â”€ Ready for model training
```

**What the Model Learns at This Stage**:
- âœ… Token frequency distributions
- âœ… Token relationships (through UIDs)
- âœ… Mathematical properties (frontend/backend numbers)
- âœ… Content similarity (content_id clustering)

**Example Output**:
```
Building SanTOK Vocabulary (60K)
============================================================
[Pass 1] Tokenizing text and counting vocabulary tokens...
  âœ“ Found 1,234,567 unique tokens
  Total token occurrences: 45,678,901
  After filtering (min_freq=2): 987,654 tokens

[Pass 2] Creating 60K vocabulary from token frequencies...
  âœ“ Vocabulary built!
  Total vocabulary size: 60,000
  Special tokens: 5
  Regular tokens: 59,995
```

---

### ğŸ¯ Phase 2: Semantic Embedding Training (Understanding)

**Purpose**: Train embeddings that capture semantic relationships between tokens.

**Step-by-Step Learning Process**:

```
1. Token Stream Preparation
   â””â”€ Load tokenized data with TokenRecords
   â””â”€ Extract UIDs, neighbors (prev_uid, next_uid)
   â””â”€ Group by stream type (char, subword, word)
       â†“
2. Vocabulary Building for Embeddings
   â””â”€ Create UID-based vocabulary
   â””â”€ Filter by minimum count
   â””â”€ Initialize random embeddings (vocab_size Ã— embedding_dim)
       â†“
3. Co-occurrence Matrix Construction
   â””â”€ Build context windows (default: Â±5 tokens)
   â””â”€ Track which tokens appear together
   â””â”€ Use SanTOK's neighbor structure:
       â€¢ prev_uid â†’ immediate predecessor
       â€¢ next_uid â†’ immediate successor
       â€¢ content_id â†’ semantic similarity
       â€¢ Same stream â†’ contextual relationships
   â””â”€ Create sparse/dense co-occurrence matrix
       â†“
4. Training Loop (Epochs)
   For each epoch:
   â”œâ”€ Positive Sampling
   â”‚  â””â”€ Sample co-occurring token pairs
   â”‚  â””â”€ Update embeddings to increase similarity
   â”‚  â””â”€ Use gradient descent:
   â”‚     â€¢ Compute dot product (similarity)
   â”‚     â€¢ Apply sigmoid activation
   â”‚     â€¢ Calculate loss (cross-entropy)
   â”‚     â€¢ Update embeddings: emb += lr * gradient
   â”‚
   â”œâ”€ Negative Sampling
   â”‚  â””â”€ Sample random non-co-occurring pairs
   â”‚  â””â”€ Update embeddings to decrease similarity
   â”‚  â””â”€ 5 negative samples per positive
   â”‚
   â””â”€ Embedding Normalization
      â””â”€ L2 normalize embeddings every 2 epochs
      â””â”€ Maintain unit vectors
       â†“
5. Model Convergence
   â””â”€ Loss decreases over epochs
   â””â”€ Embeddings capture semantic relationships
   â””â”€ Similar tokens have similar embeddings
```

**What the Model Learns**:
- âœ… **Semantic Similarity**: Tokens with similar meanings cluster together
- âœ… **Contextual Relationships**: Tokens that appear together get closer embeddings
- âœ… **Hierarchical Structure**: Multi-stream learning captures different granularities
- âœ… **Temporal Patterns**: Position-dependent semantics
- âœ… **Content Clustering**: Tokens with similar content_id cluster together

**Learning Metrics**:
```python
Epoch 1/10: Loss = 2.3456  # High loss - random embeddings
Epoch 2/10: Loss = 1.8923  # Learning patterns
Epoch 3/10: Loss = 1.4567  # Improving
Epoch 4/10: Loss = 1.1234  # Good progress
...
Epoch 10/10: Loss = 0.5678  # Converged - model learned!
```

**Visual Learning Progress**:
```
Initial State (Random):
  "cat"    â†’ [0.12, -0.45, 0.78, ...]  (random)
  "dog"    â†’ [-0.23, 0.67, -0.12, ...]  (random)
  "car"    â†’ [0.34, -0.56, 0.89, ...]  (random)
  
After Training:
  "cat"    â†’ [0.45, 0.23, 0.12, ...]  (learned)
  "dog"    â†’ [0.42, 0.25, 0.15, ...]  (similar to cat!)
  "car"    â†’ [-0.12, 0.78, -0.34, ...]  (different from cat/dog)
  
Similarity Scores:
  cat-dog:  0.87  (high - learned they're similar!)
  cat-car:  0.12  (low - learned they're different)
```

---

### ğŸš€ Phase 3: Language Model Training (Generation)

**Purpose**: Train a GPT-2 style language model to predict next tokens.

**Step-by-Step Learning Process**:

```
1. Data Preparation
   â””â”€ Encode text to token IDs using vocabulary
   â””â”€ Create sequences of fixed length (default: 512)
   â””â”€ Split into input/target pairs:
      Input:  [token_1, token_2, ..., token_n]
      Target: [token_2, token_3, ..., token_n+1]
       â†“
2. Model Architecture Initialization
   â””â”€ Token embeddings (vocab_size Ã— embedding_dim)
   â””â”€ Position embeddings (max_seq_length Ã— embedding_dim)
   â””â”€ Transformer layers (12 layers, 12 heads each):
      â€¢ Self-attention (Q, K, V projections)
      â€¢ Feed-forward networks
      â€¢ Layer normalization
   â””â”€ Output projection (embedding_dim Ã— vocab_size)
       â†“
3. Training Loop (Epochs)
   For each epoch:
   â”œâ”€ Batch Creation
   â”‚  â””â”€ Shuffle training sequences
   â”‚  â””â”€ Create batches (default: 32 sequences)
   â”‚
   â”œâ”€ Forward Pass
   â”‚  â””â”€ Embed tokens: token_emb + pos_emb
   â”‚  â””â”€ Pass through transformer layers:
   â”‚     â€¢ Multi-head self-attention
   â”‚     â€¢ Feed-forward networks
   â”‚     â€¢ Residual connections
   â”‚     â€¢ Layer normalization
   â”‚  â””â”€ Generate logits (vocab_size probabilities)
   â”‚
   â”œâ”€ Loss Calculation
   â”‚  â””â”€ Cross-entropy loss:
   â”‚     loss = -log(prob[target_token])
   â”‚  â””â”€ Average over all positions in sequence
   â”‚
   â””â”€ Weight Updates
      â””â”€ Compute gradients (backpropagation)
      â””â”€ Update all weights:
         â€¢ Embedding weights
         â€¢ Attention weights (Q, K, V, O)
         â€¢ Feed-forward weights
         â€¢ Layer norm parameters
         â€¢ Output projection
       â†“
4. Model Checkpointing
   â””â”€ Save model every N epochs
   â””â”€ Store all weights and hyperparameters
   â””â”€ Enable resuming training
       â†“
5. Convergence & Validation
   â””â”€ Loss decreases: 6.0 â†’ 2.0 â†’ 1.5 â†’ 1.2
   â””â”€ Perplexity decreases (measure of uncertainty)
   â””â”€ Model learns language patterns
```

**What the Model Learns**:
- âœ… **Next Token Prediction**: Learns to predict likely next tokens
- âœ… **Language Patterns**: Grammar, syntax, semantics
- âœ… **Context Understanding**: Uses previous tokens to predict next
- âœ… **Long-range Dependencies**: Attention mechanism captures relationships
- âœ… **Domain Knowledge**: Learns from training corpus

**Learning Progress Example**:
```
Epoch 1/10:
  Loss: 6.2345  (High - model is guessing randomly)
  Perplexity: 510.2  (Very uncertain)
  Sample: "The cat sat on the [random_word]"

Epoch 5/10:
  Loss: 2.1234  (Learning patterns)
  Perplexity: 8.4  (More confident)
  Sample: "The cat sat on the mat"  (Better!)

Epoch 10/10:
  Loss: 1.4567  (Converged)
  Perplexity: 4.3  (Confident predictions)
  Sample: "The cat sat on the mat and purred"  (Coherent!)
```

---

### ğŸ§ª Phase 4: Testing & Validation (Quality Assurance)

**Purpose**: Verify the model learned correctly and is ready for use.

**Testing Process**:

```
1. Reconstruction Testing
   â””â”€ Test: Tokenize â†’ Reconstruct
   â””â”€ Verify: Original text == Reconstructed text
   â””â”€ Metric: 100% accuracy required
       â†“
2. Embedding Quality Testing
   â””â”€ Test: Similar tokens have similar embeddings
   â””â”€ Verify: Cosine similarity > 0.7 for related tokens
   â””â”€ Metric: Semantic alignment score
       â†“
3. Language Model Testing
   â”œâ”€ Perplexity Testing
   â”‚  â””â”€ Measure model's uncertainty
   â”‚  â””â”€ Lower = better (model is confident)
   â”‚
   â”œâ”€ Generation Quality
   â”‚  â””â”€ Generate text from prompts
   â”‚  â””â”€ Check: Coherence, grammar, relevance
   â”‚
   â””â”€ Next Token Prediction Accuracy
      â””â”€ Test on held-out validation set
      â””â”€ Measure: Top-1, Top-5, Top-10 accuracy
       â†“
4. Performance Testing
   â””â”€ Speed: Tokens/second
   â””â”€ Memory: RAM usage
   â””â”€ Accuracy: Reconstruction rate
       â†“
5. Model Readiness Checklist
   âœ… Loss converged (< 2.0 for LM, < 1.0 for embeddings)
   âœ… Perplexity reasonable (< 10 for good models)
   âœ… Reconstruction accuracy = 100%
   âœ… Embedding similarity makes sense
   âœ… Generation quality acceptable
   âœ… Performance meets requirements
```

**Test Results Example**:
```
=== Model Testing Results ===

1. Reconstruction Test:
   âœ“ Accuracy: 100.0% (Perfect!)
   âœ“ All tokens correctly reconstructed

2. Embedding Quality:
   âœ“ cat-dog similarity: 0.87 (High - correct!)
   âœ“ cat-car similarity: 0.12 (Low - correct!)
   âœ“ Semantic alignment: 0.82 (Good!)

3. Language Model:
   âœ“ Perplexity: 4.3 (Excellent!)
   âœ“ Top-1 accuracy: 45.2%
   âœ“ Top-5 accuracy: 78.9%
   âœ“ Top-10 accuracy: 89.3%

4. Performance:
   âœ“ Speed: 1,234 tokens/second
   âœ“ Memory: 2.3 GB
   âœ“ Latency: 0.8ms per token

âœ… MODEL READY FOR DEPLOYMENT!
```

---

### ğŸ“Š Model Learning Indicators

**How to Know Your Model is Learning**:

1. **Loss Decreasing**:
   ```
   Epoch 1: Loss = 6.23  âŒ (Random)
   Epoch 3: Loss = 3.45  âš ï¸  (Learning)
   Epoch 5: Loss = 2.12  âœ… (Good progress)
   Epoch 10: Loss = 1.45 âœ… (Converged!)
   ```

2. **Perplexity Decreasing**:
   ```
   Epoch 1: Perplexity = 510.2  âŒ (Very uncertain)
   Epoch 5: Perplexity = 8.4    âœ… (Confident)
   Epoch 10: Perplexity = 4.3    âœ… (Very confident!)
   ```

3. **Embedding Similarity Makes Sense**:
   ```
   cat-dog: 0.87  âœ… (High - they're similar!)
   cat-car: 0.12  âœ… (Low - they're different!)
   python-code: 0.82  âœ… (High - related!)
   ```

4. **Generation Quality Improving**:
   ```
   Epoch 1: "The cat sat on the [random_word]"
   Epoch 5: "The cat sat on the mat"
   Epoch 10: "The cat sat on the mat and purred contentedly"
   ```

5. **Reconstruction Accuracy**:
   ```
   Always: 100% âœ… (SanTOK guarantees perfect reconstruction)
   ```

---

### ğŸ¯ Model Readiness Checklist

**Your model is ready when**:

- âœ… **Loss converged**: Loss < 2.0 (language model) or < 1.0 (embeddings)
- âœ… **Perplexity reasonable**: < 10 for good models, < 5 for excellent
- âœ… **Reconstruction perfect**: 100% accuracy (always true for SanTOK)
- âœ… **Embedding quality**: Similarity scores make semantic sense
- âœ… **Generation coherent**: Generated text is grammatically correct
- âœ… **Performance acceptable**: Meets speed/memory requirements
- âœ… **Validation passed**: All tests pass

**Model NOT ready if**:
- âŒ Loss = 0.0000 (trivial memorization - dataset too small)
- âŒ Loss not decreasing (learning rate too high/low)
- âŒ Perplexity > 100 (model is guessing randomly)
- âŒ Generation is gibberish
- âŒ Embeddings don't capture semantics

---

### ğŸ”¬ Advanced Training Features

**Enhanced Semantic Trainer** (Multi-Stream Learning):

```
Multiple Token Streams
    â”œâ”€ char stream (character-level)
    â”œâ”€ subword stream (subword-level)
    â””â”€ word stream (word-level)
        â†“
[Multi-Stream Learning]
    â”œâ”€ Learn at all granularities simultaneously
    â”œâ”€ Cross-stream alignment
    â”œâ”€ Hierarchical semantics
    â””â”€ Unified embeddings
        â†“
Enhanced Embeddings
    â””â”€ Capture semantics at multiple levels
    â””â”€ Better generalization
    â””â”€ Richer representations
```

**Key Learning Mechanisms**:

1. **Co-occurrence Learning**: Tokens appearing together get similar embeddings
2. **Negative Sampling**: Random tokens get pushed apart
3. **Gradient Descent**: Iteratively improve embeddings
4. **Normalization**: Maintain unit vectors for stability
5. **Multi-Stream Alignment**: Align semantics across granularities
6. **Temporal Patterns**: Learn position-dependent semantics
7. **Content Clustering**: Group tokens by content_id

---

### ğŸ”„ Complete Training Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Model Development Workflow                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 1: Data Preparation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Raw Text Corpus
    â†“
[SanTOK Tokenization]
    â”œâ”€ Extract tokens
    â”œâ”€ Generate UIDs
    â”œâ”€ Calculate features
    â””â”€ Create TokenRecords
    â†“
Tokenized Dataset
    â†“

PHASE 2: Vocabulary Building
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tokenized Dataset
    â†“
[Vocabulary Builder]
    â”œâ”€ Count token frequencies
    â”œâ”€ Filter by min_frequency
    â”œâ”€ Select top 60K tokens
    â”œâ”€ Assign token IDs
    â””â”€ Save vocabulary
    â†“
Vocabulary File (60K tokens)
    â†“

PHASE 3A: Semantic Embedding Training
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tokenized Dataset + Vocabulary
    â†“
[Semantic Trainer]
    â”œâ”€ Build co-occurrence matrix
    â”‚  â””â”€ Use prev_uid, next_uid
    â”‚  â””â”€ Context windows
    â”‚  â””â”€ Content_id similarity
    â”œâ”€ Initialize random embeddings
    â”œâ”€ Training Loop (10 epochs):
    â”‚  â”œâ”€ Positive sampling
    â”‚  â”œâ”€ Negative sampling
    â”‚  â”œâ”€ Gradient updates
    â”‚  â””â”€ Embedding normalization
    â””â”€ Save trained embeddings
    â†“
Trained Embeddings Model
    â†“

PHASE 3B: Language Model Training
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tokenized Dataset + Vocabulary
    â†“
[Language Model Trainer]
    â”œâ”€ Encode to token IDs
    â”œâ”€ Create sequences
    â”œâ”€ Initialize transformer model
    â”œâ”€ Training Loop (10 epochs):
    â”‚  â”œâ”€ Forward pass
    â”‚  â”œâ”€ Loss calculation
    â”‚  â”œâ”€ Backpropagation
    â”‚  â””â”€ Weight updates
    â””â”€ Save trained model
    â†“
Trained Language Model
    â†“

PHASE 4: Testing & Validation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Trained Models
    â†“
[Testing Suite]
    â”œâ”€ Reconstruction test
    â”œâ”€ Embedding quality test
    â”œâ”€ Generation quality test
    â”œâ”€ Performance benchmarks
    â””â”€ Validation metrics
    â†“
Test Results Report
    â†“

PHASE 5: Model Deployment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validated Models
    â†“
[Deployment]
    â”œâ”€ Load models
    â”œâ”€ Initialize inference pipeline
    â”œâ”€ API server (optional)
    â””â”€ Production ready!
    â†“
ğŸš€ DEPLOYED MODEL
```

---

## ğŸ¯ How SanTOK Components Help Your Model - Practical Benefits

This section showcases **exactly how each SanTOK component contributes** to making your models better, faster, and more reliable.

---

### ğŸ”¤ How SanTOK Tokenization Helps Your Model

**What SanTOK Tokenization Provides**:
- âœ… Deterministic UIDs (same token = same UID always)
- âœ… Mathematical properties (frontend digits, backend numbers)
- âœ… Multiple granularities (char, subword, word)
- âœ… Perfect reversibility (100% reconstruction)
- âœ… Statistical features (entropy, balance, variance)

**How It Helps Your Model**:

#### 1. **Deterministic Foundation**
```
Without SanTOK:
  "cat" â†’ random ID (different each time)
  Model confusion: Same word, different IDs

With SanTOK:
  "cat" â†’ UID: 12345 (always the same)
  Model benefit: Consistent representation = better learning
```

**Impact**: Models learn faster because tokens have stable identities. No confusion from changing IDs.

#### 2. **Mathematical Relationships**
```
SanTOK provides:
  - Frontend digit (1-9): Semantic category
  - Backend number: Positional encoding
  - Global ID: Full context signature

Model uses these for:
  - Better feature engineering
  - Mathematical relationships between tokens
  - Deterministic clustering
```

**Impact**: Models can leverage mathematical properties for better understanding, not just raw text.

#### 3. **Multiple Granularities**
```
SanTOK provides 3 streams simultaneously:
  - Character level: "c", "a", "t"
  - Subword level: "cat"
  - Word level: "cat"

Model benefits:
  - Learn at all levels simultaneously
  - Better handling of rare words
  - Richer representations
```

**Impact**: Models understand text at multiple levels, improving generalization.

#### 4. **Perfect Reversibility**
```
SanTOK guarantee:
  Tokenize â†’ Reconstruct = 100% accuracy

Model benefit:
  - No information loss
  - Can verify correctness
  - Trustworthy pipeline
```

**Impact**: Models built on SanTOK are reliable and verifiable.

**Real Example**:
```python
# Without SanTOK: Standard tokenizer
text = "Hello world"
tokens = ["Hello", "world"]  # Lost capitalization, punctuation info

# With SanTOK: Rich tokenization
text = "Hello world"
tokens = [
    TokenRecord(text="Hello", uid=12345, frontend=5, backend=678, ...),
    TokenRecord(text="world", uid=23456, frontend=6, backend=789, ...)
]
# Model gets: text + UID + mathematical properties + neighbors
```

**Result**: Model has **10x more information** per token, leading to better learning.

---

### ğŸ§® How SanTOK Embeddings Help Your Model

**What SanTOK Embeddings Provide**:
- âœ… Feature-based embeddings (from SanTOK properties)
- âœ… Semantic embeddings (self-trained, no external models)
- âœ… Hash-based embeddings (fast, deterministic)
- âœ… Hybrid embeddings (combines multiple strategies)

**How It Helps Your Model**:

#### 1. **No External Dependencies**
```
Without SanTOK:
  Model needs: BERT, Word2Vec, Sentence Transformers
  Problems: Large models, slow, requires internet

With SanTOK:
  Model gets: Self-trained embeddings from your data
  Benefits: Fast, lightweight, works offline
```

**Impact**: Models can be trained and deployed anywhere, no external dependencies.

#### 2. **Domain-Specific Learning**
```
SanTOK embeddings learn from YOUR data:
  - Medical text â†’ medical embeddings
  - Code â†’ code embeddings
  - Your domain â†’ your embeddings

Standard embeddings:
  - Generic (trained on Wikipedia)
  - May not fit your domain
```

**Impact**: Models perform better on domain-specific tasks because embeddings match the domain.

#### 3. **Mathematical Property Integration**
```
SanTOK embeddings include:
  - UID relationships
  - Frontend/backend numbers
  - Content_id clustering
  - Neighbor relationships

Standard embeddings:
  - Only text similarity
  - No mathematical structure
```

**Impact**: Models can use mathematical relationships for better reasoning.

#### 4. **Multiple Embedding Strategies**
```
SanTOK provides 4 strategies:
  1. Feature-based: Fast, deterministic
  2. Semantic: Learned from data
  3. Hash-based: Ultra-fast, no training
  4. Hybrid: Best of all worlds

Model can choose based on:
  - Speed requirements
  - Accuracy needs
  - Resource constraints
```

**Impact**: Models can optimize for speed or accuracy as needed.

**Real Example**:
```python
# Standard embedding: Generic, slow
embedding = sentence_transformer.encode("cat")
# Time: 50ms, Size: 384 dim, Generic semantics

# SanTOK embedding: Domain-specific, fast
embedding = santok_embedding.generate("cat")
# Time: 2ms, Size: 768 dim, Your domain semantics
# Includes: UID relationships, mathematical properties
```

**Result**: Models get **faster, more accurate, domain-specific embeddings**.

---

### ğŸ§  How SanTOK Semantics Help Your Model

**What SanTOK Semantics Provide**:
- âœ… Self-supervised semantic learning
- âœ… Co-occurrence patterns
- âœ… Context windows
- âœ… Content-based clustering
- âœ… Multi-stream alignment

**How It Helps Your Model**:

#### 1. **Learns from Your Data**
```
SanTOK semantics:
  - Analyzes YOUR text corpus
  - Learns relationships in YOUR domain
  - Captures YOUR terminology

Standard semantics:
  - Pre-trained on generic data
  - May not understand your domain
```

**Impact**: Models understand your specific domain better.

#### 2. **Captures Contextual Relationships**
```
SanTOK learns:
  - Which tokens appear together
  - Context windows (neighbors)
  - Sequential patterns
  - Temporal relationships

Model uses for:
  - Better next-token prediction
  - Understanding context
  - Capturing dependencies
```

**Impact**: Models understand context and relationships, not just individual tokens.

#### 3. **Multi-Stream Semantic Alignment**
```
SanTOK provides:
  - Character-level semantics
  - Subword-level semantics
  - Word-level semantics
  - Cross-stream alignment

Model benefits:
  - Understands at all granularities
  - Better handling of rare words
  - Richer semantic understanding
```

**Impact**: Models have deeper semantic understanding across multiple levels.

#### 4. **Deterministic Semantic Graph**
```
SanTOK creates:
  - Persistent semantic relationships
  - UID-based semantic graph
  - Content_id clusters
  - Temporal patterns

Model uses for:
  - Consistent semantic understanding
  - Better generalization
  - Explainable semantics
```

**Impact**: Models have consistent, explainable semantic understanding.

**Real Example**:
```python
# Standard semantics: Generic
"Python" â†’ generic programming language embedding

# SanTOK semantics: Domain-aware
"Python" â†’ embedding that includes:
  - Co-occurrence with "code", "programming", "language"
  - Content_id cluster (programming concepts)
  - Temporal patterns (appears with "develop", "script")
  - Multi-stream alignment (char/subword/word levels)
```

**Result**: Models have **richer, domain-specific semantic understanding**.

---

### ğŸ’¾ How SanTOK Vectors Help Your Model

**What SanTOK Vectors Provide**:
- âœ… Unified vector store interface
- âœ… Multiple backends (ChromaDB, FAISS, Weaviate)
- âœ… Efficient similarity search
- âœ… Metadata management
- âœ… Batch operations

**How It Helps Your Model**:

#### 1. **Fast Similarity Search**
```
SanTOK vectors:
  - Optimized similarity search
  - Sub-millisecond queries
  - Scales to millions of vectors

Standard approach:
  - Linear search (slow)
  - No optimization
  - Doesn't scale
```

**Impact**: Models can quickly find similar examples for few-shot learning, retrieval, etc.

#### 2. **Multiple Storage Backends**
```
SanTOK supports:
  - ChromaDB: Persistent, disk-based
  - FAISS: Fast, in-memory
  - Weaviate: Cloud-native, scalable

Model can choose:
  - Development: FAISS (fast)
  - Production: Weaviate (scalable)
  - Local: ChromaDB (simple)
```

**Impact**: Models can use the best storage for their use case.

#### 3. **Metadata Integration**
```
SanTOK vectors store:
  - Embeddings
  - Token metadata (UID, frontend, backend)
  - Source information
  - Timestamps
  - Custom tags

Model uses for:
  - Filtered searches
  - Source tracking
  - Temporal queries
```

**Impact**: Models can do sophisticated queries beyond simple similarity.

#### 4. **Batch Operations**
```
SanTOK vectors:
  - Batch insert (thousands at once)
  - Batch search (multiple queries)
  - Efficient updates

Standard approach:
  - One-by-one operations
  - Slow for large datasets
```

**Impact**: Models can efficiently work with large datasets.

**Real Example**:
```python
# Standard approach: Slow linear search
similar = find_similar(embedding)  # O(n) - scans all vectors

# SanTOK vectors: Fast indexed search
similar = vector_store.search(embedding, top_k=10)  # O(log n) - indexed
# Returns: Similar vectors + metadata + source info
```

**Result**: Models get **10-100x faster similarity search** with rich metadata.

---

### ğŸŒ³ How SanTOK Trees, Graphs & Reasoning Help Your Model

**What SanTOK Trees, Graphs & Reasoning Provide**:
- âœ… Knowledge graphs (nodes, edges, relations)
- âœ… Hierarchical trees (concepts, documents)
- âœ… Symbolic reasoning (20+ inference rules)
- âœ… Contradiction detection
- âœ… Confidence propagation

**How It Helps Your Model**:

#### 1. **Structured Knowledge**
```
SanTOK provides:
  - Knowledge graph: Relationships between concepts
  - Trees: Hierarchical organization
  - Reasoning: Logical inference

Model uses for:
  - Understanding relationships
  - Hierarchical concepts
  - Logical reasoning
```

**Impact**: Models can reason about structured knowledge, not just text.

#### 2. **Explainable Reasoning**
```
SanTOK reasoning:
  - Shows inference steps
  - Explains conclusions
  - Tracks confidence

Standard models:
  - Black box predictions
  - No explanation
  - Unclear reasoning
```

**Impact**: Models can explain their reasoning, crucial for trust and debugging.

#### 3. **Contradiction Detection**
```
SanTOK detects:
  - Conflicting information
  - Logical inconsistencies
  - Confidence conflicts

Model uses for:
  - Validating outputs
  - Preventing hallucinations
  - Ensuring consistency
```

**Impact**: Models can catch and prevent errors before they happen.

#### 4. **20+ Inference Rules**
```
SanTOK provides:
  - Transitivity: Aâ†’B, Bâ†’C â†’ Aâ†’C
  - Inheritance: IS_A relationships
  - Symmetry: Bidirectional relations
  - And 17+ more rules

Model uses for:
  - Logical inference
  - Knowledge expansion
  - Relationship discovery
```

**Impact**: Models can make logical inferences, expanding knowledge automatically.

**Real Example**:
```python
# Standard model: Text-only
Question: "Is Python a programming language?"
Answer: "Yes" (but can't explain why)

# SanTOK reasoning: Structured knowledge
Question: "Is Python a programming language?"
Reasoning:
  1. Python IS_A programming language (fact)
  2. Therefore: Yes
  3. Confidence: 1.0 (certain)
  4. Explanation: Direct IS_A relationship
```

**Result**: Models can **reason logically and explain their answers**.

---

### ğŸ§  How SanTOK Cognitive Helps Your Model

**What SanTOK Cognitive Provides**:
- âœ… Deterministic reasoning substrate
- âœ… Knowledge representation
- âœ… Symbolic inference
- âœ… Constraint enforcement
- âœ… Full explainability

**How It Helps Your Model**:

#### 1. **System 2 for AI**
```
SanTOK Cognitive = System 2 (deliberate, correct)
LLMs = System 1 (fast, intuitive, error-prone)

Combined:
  - LLM generates fast responses
  - SanTOK validates and corrects
  - Best of both worlds
```

**Impact**: Models get the speed of LLMs with the correctness of symbolic reasoning.

#### 2. **Hallucination Prevention**
```
SanTOK Cognitive:
  - Validates against knowledge graph
  - Checks for contradictions
  - Enforces constraints

Standard models:
  - Can hallucinate
  - No validation
  - Unreliable outputs
```

**Impact**: Models produce reliable, validated outputs.

#### 3. **Constraint Enforcement**
```
SanTOK Cognitive:
  - Enforces domain constraints
  - Validates against rules
  - Prevents invalid outputs

Model uses for:
  - Safe generation
  - Compliance
  - Quality assurance
```

**Impact**: Models stay within safe, valid boundaries.

#### 4. **Full Explainability**
```
SanTOK Cognitive:
  - Shows reasoning trace
  - Explains every step
  - Provides confidence scores

Standard models:
  - Black box
  - No explanation
  - Unclear reasoning
```

**Impact**: Models are trustworthy and debuggable.

**Real Example**:
```python
# Standard LLM: Can hallucinate
Question: "What is the capital of France?"
Answer: "Paris" (correct, but can't explain)

# SanTOK Cognitive: Validated and explained
Question: "What is the capital of France?"
Answer: "Paris"
Reasoning:
  - France HAS_CAPITAL Paris (fact in knowledge graph)
  - Confidence: 1.0
  - Source: Knowledge graph node #12345
  - Validation: âœ“ No contradictions found
```

**Result**: Models are **reliable, explainable, and validated**.

---

### ğŸ¤– How SanTOK SLM (Small Language Model) Helps Your Model

**What SanTOK SLM Provides**:
- âœ… 100% SanTOK-native (no external AI)
- âœ… Constraint-grounded generation
- âœ… No hallucination
- âœ… Full explainability
- âœ… Lightweight and fast

**How It Helps Your Model**:

#### 1. **No External Dependencies**
```
SanTOK SLM:
  - Uses only SanTOK components
  - No BERT, GPT, or other models
  - Pure SanTOK tokenization + embeddings

Standard SLMs:
  - Require external models
  - Large dependencies
  - Complex setup
```

**Impact**: Models are self-contained and easy to deploy.

#### 2. **Constraint-Grounded Generation**
```
SanTOK SLM:
  - Generates within constraints
  - Validates against knowledge graph
  - Prevents invalid outputs

Standard SLMs:
  - Can generate anything
  - No validation
  - Unreliable
```

**Impact**: Models generate safe, valid outputs.

#### 3. **No Hallucination**
```
SanTOK SLM:
  - Only generates from learned knowledge
  - Validates against facts
  - No made-up information

Standard SLMs:
  - Can hallucinate
  - Make up facts
  - Unreliable
```

**Impact**: Models are trustworthy and factual.

#### 4. **Lightweight and Fast**
```
SanTOK SLM:
  - Small model size
  - Fast inference
  - Low memory usage

Standard SLMs:
  - Large models
  - Slow inference
  - High memory
```

**Impact**: Models can run on edge devices, mobile, etc.

**Real Example**:
```python
# Standard SLM: Large, slow, can hallucinate
model = load_pretrained_slm()  # 500MB, slow, unreliable

# SanTOK SLM: Small, fast, reliable
model = SanTOKSLMModel()  # 50MB, fast, validated
# Generates: Constraint-grounded, explainable, no hallucination
```

**Result**: Models are **lightweight, fast, and reliable**.

---

### ğŸ§  How SanTOK Memory Helps Your Model

**What SanTOK Memory Provides**:
- âœ… Unified memory system
- âœ… Vector + Graph + Tree storage
- âœ… Cross-store linking
- âœ… Temporal tracking
- âœ… Source awareness

**How It Helps Your Model**:

#### 1. **Unified Knowledge Storage**
```
SanTOK Memory:
  - Vector store: Similarity search
  - Graph store: Relationships
  - Tree store: Hierarchies
  - All linked together

Standard approach:
  - Separate stores
  - No integration
  - Fragmented knowledge
```

**Impact**: Models have unified, integrated knowledge.

#### 2. **Multi-Modal Retrieval**
```
SanTOK Memory:
  - Search by similarity (vector)
  - Search by relationship (graph)
  - Search by hierarchy (tree)
  - Combined searches

Standard approach:
  - Single search type
  - Limited retrieval
```

**Impact**: Models can retrieve knowledge in multiple ways.

#### 3. **Temporal Awareness**
```
SanTOK Memory:
  - Tracks when knowledge was added
  - Temporal relationships
  - Freshness scoring

Model uses for:
  - Recent information priority
  - Temporal reasoning
  - Knowledge evolution
```

**Impact**: Models understand time and can prioritize recent information.

#### 4. **Source Tracking**
```
SanTOK Memory:
  - Tracks source of each fact
  - Source-aware queries
  - Source-based filtering

Model uses for:
  - Attribution
  - Source verification
  - Quality control
```

**Impact**: Models can cite sources and verify information.

**Real Example**:
```python
# Standard memory: Single store
memory = vector_store  # Only similarity search

# SanTOK Memory: Unified system
memory = UnifiedMemory()
memory.add("Python is a language", source="wikipedia")
# Stored in: Vector store + Graph store + Tree store
# Linked together, with source tracking
```

**Result**: Models have **unified, multi-modal, source-aware memory**.

---

### ğŸ” How SanTOK Interpretation Helps Your Model

**What SanTOK Interpretation Provides**:
- âœ… Real-time data interpretation
- âœ… Semantic relationship discovery
- âœ… Concept exploration
- âœ… Knowledge discovery
- âœ… Weaviate integration

**How It Helps Your Model**:

#### 1. **Real-Time Understanding**
```
SanTOK Interpretation:
  - Interprets data as it arrives
  - Finds semantic relationships
  - Discovers concepts

Standard approach:
  - Batch processing
  - No real-time understanding
```

**Impact**: Models can understand and interpret data in real-time.

#### 2. **Semantic Relationship Discovery**
```
SanTOK Interpretation:
  - Finds related concepts
  - Discovers relationships
  - Builds knowledge graph

Model uses for:
  - Understanding context
  - Relationship discovery
  - Knowledge expansion
```

**Impact**: Models can discover and understand relationships automatically.

#### 3. **Concept Exploration**
```
SanTOK Interpretation:
  - Explores concepts deeply
  - Multi-level exploration
  - Hierarchical understanding

Model uses for:
  - Deep understanding
  - Concept hierarchies
  - Knowledge navigation
```

**Impact**: Models can explore and understand concepts at multiple levels.

#### 4. **Knowledge Discovery**
```
SanTOK Interpretation:
  - Discovers new knowledge
  - Finds patterns
  - Builds understanding

Standard approach:
  - Static knowledge
  - No discovery
```

**Impact**: Models can discover new knowledge from data.

**Real Example**:
```python
# Standard approach: Static processing
data = "Machine learning uses neural networks"
result = process(data)  # Basic processing

# SanTOK Interpretation: Dynamic understanding
data = "Machine learning uses neural networks"
result = interpreter.interpret(data)
# Discovers:
#   - "machine learning" IS_A "AI technique"
#   - "neural networks" USES "machine learning"
#   - Related concepts: "deep learning", "training"
#   - Builds knowledge graph automatically
```

**Result**: Models can **discover and understand knowledge dynamically**.

---

### ğŸ¯ Combined Impact: How All Components Work Together

**The Complete SanTOK Advantage**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         How SanTOK Components Help Your Model                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Tokenization
   â””â”€ Provides: Deterministic foundation, mathematical properties
   â””â”€ Helps Model: Stable learning, rich features

2. Embeddings
   â””â”€ Provides: Domain-specific, fast embeddings
   â””â”€ Helps Model: Better representations, no external deps

3. Semantics
   â””â”€ Provides: Self-learned semantic understanding
   â””â”€ Helps Model: Domain-aware, contextual understanding

4. Vectors
   â””â”€ Provides: Fast similarity search, multiple backends
   â””â”€ Helps Model: Efficient retrieval, scalable storage

5. Trees & Graphs
   â””â”€ Provides: Structured knowledge, reasoning
   â””â”€ Helps Model: Logical inference, explainability

6. Cognitive
   â””â”€ Provides: Validation, constraint enforcement
   â””â”€ Helps Model: Reliable outputs, no hallucination

7. SLM
   â””â”€ Provides: Lightweight, constraint-grounded generation
   â””â”€ Helps Model: Fast, reliable text generation

8. Memory
   â””â”€ Provides: Unified, multi-modal knowledge storage
   â””â”€ Helps Model: Integrated knowledge, source tracking

9. Interpretation
   â””â”€ Provides: Real-time understanding, knowledge discovery
   â””â”€ Helps Model: Dynamic learning, relationship discovery

COMBINED RESULT:
âœ… Faster training (deterministic, rich features)
âœ… Better accuracy (domain-specific, validated)
âœ… More reliable (no hallucination, constraints)
âœ… Fully explainable (reasoning traces, sources)
âœ… Production-ready (scalable, efficient)
âœ… Self-contained (no external dependencies)
```

---

## ğŸ“ Training & Testing Architecture - Clean Face

### Overview

SanTOK provides comprehensive training and testing systems for semantic models, language models, and performance evaluation.

### Training Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Training System Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Data Preparation                              â”‚    â”‚
â”‚  â”‚  - Corpus loading                                     â”‚    â”‚
â”‚  â”‚  - Text preprocessing                                 â”‚    â”‚
â”‚  â”‚  - Dataset splitting (train/val/test)                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Tokenization                                 â”‚    â”‚
â”‚  â”‚  - TextTokenizer.build()                             â”‚    â”‚
â”‚  â”‚  - Multiple streams                                 â”‚    â”‚
â”‚  â”‚  - TokenRecord creation                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Training Type Selection                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Semantic     â”‚  â”‚ Language     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Embedding    â”‚  â”‚ Model        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Training     â”‚  â”‚ Training     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Enhanced     â”‚  â”‚ Vocabulary   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Semantic     â”‚  â”‚ Building     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Training     â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Training Execution                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ For each epoch:                      â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  1. Sample batch                     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  2. Forward pass                     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  3. Compute loss                      â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  4. Backward pass                     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  5. Update parameters                â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  6. Validation (if applicable)       â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Model Evaluation                              â”‚    â”‚
â”‚  â”‚  - Loss curves                                        â”‚    â”‚
â”‚  â”‚  - Embedding quality metrics                          â”‚    â”‚
â”‚  â”‚  - Similarity evaluation                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Model Persistence                             â”‚    â”‚
â”‚  â”‚  - Save embeddings                                   â”‚    â”‚
â”‚  â”‚  - Save vocabulary                                   â”‚    â”‚
â”‚  â”‚  - Save metadata                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Testing Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Testing System Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Test Categories                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Unit Tests   â”‚  â”‚ Integration  â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Functions  â”‚  â”‚ Tests        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Classes    â”‚  â”‚ - Pipelines  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Performance â”‚  â”‚ Accuracy     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Tests        â”‚  â”‚ Tests        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Speed      â”‚  â”‚ - Correctnessâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Memory     â”‚  â”‚ - Quality    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Test Execution                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ 1. Tokenization Tests                â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - All 9 methods                   â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Determinism                     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Mathematical properties          â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ 2. Embedding Tests                   â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - All 4 strategies                â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Dimension correctness           â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Normalization                   â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ 3. Training Tests                    â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Vocabulary building             â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Training convergence           â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Model saving/loading            â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ 4. Performance Benchmarks            â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Speed measurements              â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Memory usage                    â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    - Scalability                     â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Test Reporting                               â”‚    â”‚
â”‚  â”‚  - Pass/Fail status                                 â”‚    â”‚
â”‚  â”‚  - Performance metrics                              â”‚    â”‚
â”‚  â”‚  - Coverage reports                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Workflow

```
1. Data Collection
    â†“
2. Preprocessing
    â†“
3. Tokenization
    â†“
4. Vocabulary Building
    â†“
5. Co-occurrence Matrix
    â†“
6. Model Initialization
    â†“
7. Training Loop
    â”œâ”€ Epoch 1 â†’ Loss: 0.5
    â”œâ”€ Epoch 2 â†’ Loss: 0.4
    â”œâ”€ Epoch 3 â†’ Loss: 0.3
    â””â”€ ...
    â†“
8. Validation
    â†“
9. Model Saving
    â†“
10. Evaluation
```

---

## ğŸŒ³ Trees, Graphs & Reasoning Architecture - Clean Face

### Overview

SanTOK Cognitive provides a complete knowledge representation and reasoning system using trees, graphs, and symbolic inference.

### Knowledge Trees Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Knowledge Trees Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Tree Structure                              â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚              Root Node                               â”‚    â”‚
â”‚  â”‚              (depth=0)                               â”‚    â”‚
â”‚  â”‚                 â”‚                                     â”‚    â”‚
â”‚  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚    â”‚
â”‚  â”‚        â”‚        â”‚        â”‚                           â”‚    â”‚
â”‚  â”‚    Child 1   Child 2   Child 3                      â”‚    â”‚
â”‚  â”‚   (depth=1)  (depth=1)  (depth=1)                    â”‚    â”‚
â”‚  â”‚        â”‚        â”‚        â”‚                           â”‚    â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”´â”€â”€â”€â”    â”‚    â”Œâ”€â”€â”€â”´â”€â”€â”€â”                       â”‚    â”‚
â”‚  â”‚    â”‚       â”‚    â”‚    â”‚       â”‚                       â”‚    â”‚
â”‚  â”‚  Leaf 1  Leaf 2â”‚  Leaf 3  Leaf 4                    â”‚    â”‚
â”‚  â”‚                â”‚                                     â”‚    â”‚
â”‚  â”‚            Leaf 5                                    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â”‚  TreeNode Properties:                                        â”‚
â”‚    - node_id: Unique identifier                             â”‚
â”‚    - content: Text/label                                    â”‚
â”‚    - parent_id: Parent node reference                      â”‚
â”‚    - children_ids: List of child node IDs                  â”‚
â”‚    - depth: Hierarchical depth                             â”‚
â”‚    - metadata: Additional properties                       â”‚
â”‚    - embedding_ref: Link to vector store                   â”‚
â”‚    - graph_node_ref: Link to graph store                   â”‚
â”‚                                                               â”‚
â”‚  Operations:                                                 â”‚
â”‚    - add_node(): Add new node                              â”‚
â”‚    - remove_node(): Remove node (recursive)                â”‚
â”‚    - get_path(): Get path from root to node                â”‚
â”‚    - traverse_dfs(): Depth-first traversal                â”‚
â”‚    - traverse_bfs(): Breadth-first traversal              â”‚
â”‚    - get_subtree(): Extract subtree                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Knowledge Graph Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Knowledge Graph Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Graph Structure                              â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  Node A â”€â”€IS_Aâ”€â”€â†’ Node B                             â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚ PART_OF         USES                                 â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚    â†“                â†“                                 â”‚    â”‚
â”‚  â”‚  Node C â”€â”€CAUSESâ”€â”€â†’ Node D                            â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚ LOCATED_IN      RELATED_TO                            â”‚    â”‚
â”‚  â”‚    â”‚                â”‚                                 â”‚    â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â”‚  GraphNode Properties:                                       â”‚
â”‚    - id: Unique integer ID                                  â”‚
â”‚    - label: Human-readable label                           â”‚
â”‚    - properties: Dictionary of properties                  â”‚
â”‚    - edges: List of outgoing edges                         â”‚
â”‚                                                               â”‚
â”‚  GraphEdge Properties:                                       â”‚
â”‚    - source_id: Source node ID                             â”‚
â”‚    - target_id: Target node ID                             â”‚
â”‚    - relation: RelationType (15+ types)                    â”‚
â”‚    - confidence: Confidence score (0-1)                    â”‚
â”‚    - metadata: Additional properties                       â”‚
â”‚                                                               â”‚
â”‚  Relation Types (15+):                                       â”‚
â”‚    - IS_A, PART_OF, CAUSES, USES                           â”‚
â”‚    - LOCATED_IN, RELATED_TO, PRECEDES                      â”‚
â”‚    - OPPOSITE_OF, SIMILAR_TO, CONTAINS                     â”‚
â”‚    - ... (and more)                                        â”‚
â”‚                                                               â”‚
â”‚  Operations:                                                 â”‚
â”‚    - add_node(): Add new node                              â”‚
â”‚    - add_edge(): Add relation                              â”‚
â”‚    - get_neighbors(): Get connected nodes                  â”‚
â”‚    - find_path(): Find path between nodes                 â”‚
â”‚    - query(): Complex graph queries                        â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reasoning Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Reasoning Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Query: "What is machine learning?"                         â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Query Parser                                 â”‚    â”‚
â”‚  â”‚  - Parse natural language                            â”‚    â”‚
â”‚  â”‚  - Extract key concepts                              â”‚    â”‚
â”‚  â”‚  - Build structured query                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Knowledge Retrieval                          â”‚    â”‚
â”‚  â”‚  - Search in GraphStore                              â”‚    â”‚
â”‚  â”‚  - Search in TreeStore                               â”‚    â”‚
â”‚  â”‚  - Search in UnifiedMemory                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Inference Engine                              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Apply Inference Rules (20+)          â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Transitivity: Aâ†’B, Bâ†’C â†’ Aâ†’C     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Inheritance: IS_A relationships   â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Symmetry: Aâ†”B                     â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Inverse: Aâ†’B â†’ Bâ†A                â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - ... (16+ more rules)              â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚                 â†“                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Rule Chaining                        â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Chain multiple rules              â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Propagate confidence              â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Track reasoning path              â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚                 â†“                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Generate Inferred Facts              â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - New relationships                 â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Confidence scores                 â”‚           â”‚    â”‚
â”‚  â”‚  â”‚  - Reasoning traces                  â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Path Finding                                 â”‚    â”‚
â”‚  â”‚  - Find reasoning paths                              â”‚    â”‚
â”‚  â”‚  - Calculate path confidence                         â”‚    â”‚
â”‚  â”‚  - Rank paths by relevance                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Contradiction Detection                      â”‚    â”‚
â”‚  â”‚  - Check for conflicting facts                       â”‚    â”‚
â”‚  â”‚  - Flag contradictions                               â”‚    â”‚
â”‚  â”‚  - Resolve conflicts (if possible)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Explanation Generation                        â”‚    â”‚
â”‚  â”‚  - Build reasoning trace                             â”‚    â”‚
â”‚  â”‚  - Format explanation                                â”‚    â”‚
â”‚  â”‚  - Include confidence scores                         â”‚    â”‚
â”‚  â”‚  - Link to source facts                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  Answer with Full Reasoning Trace                          â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Unified Memory Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Unified Memory Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         MemoryObject                                 â”‚    â”‚
â”‚  â”‚  - content: Text/fact                                â”‚    â”‚
â”‚  â”‚  - type: "fact", "concept", "rule", etc.            â”‚    â”‚
â”‚  â”‚  - metadata: Additional properties                  â”‚    â”‚
â”‚  â”‚  - graph_node_ref: Link to graph                    â”‚    â”‚
â”‚  â”‚  - tree_node_ref: Link to tree                       â”‚    â”‚
â”‚  â”‚  - embedding_ref: Link to vector store               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Auto-Linking                                 â”‚    â”‚
â”‚  â”‚  When auto_link_graph=True:                         â”‚    â”‚
â”‚  â”‚    - Extract entities from content                   â”‚    â”‚
â”‚  â”‚    - Create graph nodes                              â”‚    â”‚
â”‚  â”‚    - Create relations (IS_A, PART_OF, etc.)         â”‚    â”‚
â”‚  â”‚    - Link memory object to graph                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Storage Integration                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ GraphStore   â”‚  â”‚ TreeStore    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ (Relations)  â”‚  â”‚ (Hierarchy)  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚    â”‚
â”‚  â”‚  â”‚ VectorStore  â”‚                                   â”‚    â”‚
â”‚  â”‚  â”‚ (Embeddings) â”‚                                   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reasoning Flow Example

```
Query: "What is Python?"
    â†“
[Query Parser]
    Extract: "Python" (entity)
    Query type: DEFINITION
    â†“
[Knowledge Retrieval]
    Find in GraphStore:
        Node: "Python" (id=1)
        Edge: Python --IS_A--> Programming Language
    Find in Memory:
        MemoryObject: "Python is a programming language"
    â†“
[Inference Engine]
    Apply rules:
        - IS_A transitivity
        - Inheritance
    Generate inferred facts:
        Python IS_A Programming Language
        Programming Language IS_A Software Tool
        â†’ Python IS_A Software Tool (inferred)
    â†“
[Path Finding]
    Find paths:
        Path 1: Python â†’ Programming Language (confidence: 1.0)
        Path 2: Python â†’ Programming Language â†’ Software Tool (confidence: 0.9)
    â†“
[Explanation Generation]
    Build trace:
        Facts used: 2
        Rules applied: transitive_is_a
        Path: Python â†’ Programming Language
        Confidence: 95%
    â†“
Answer: "Python is a type of programming language."
Explanation: [Full reasoning trace]
```

---

## ğŸŒ API Server Architecture - Clean Face

### Overview

SanTOK provides production-ready API servers built on FastAPI, supporting REST endpoints, WebSocket connections, file uploads, and async job processing.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK API Server Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Client Layer                             â”‚    â”‚
â”‚  â”‚  - HTTP Clients (REST)                               â”‚    â”‚
â”‚  â”‚  - WebSocket Clients                                 â”‚    â”‚
â”‚  â”‚  - File Upload Clients                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              FastAPI Application                      â”‚    â”‚
â”‚  â”‚  - FastAPI app instance                              â”‚    â”‚
â”‚  â”‚  - Route registration                                â”‚    â”‚
â”‚  â”‚  - Middleware stack                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Middleware Layer                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ CORS         â”‚â†’ â”‚ Authenticationâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Handler      â”‚  â”‚ (JWT)         â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Request      â”‚â†’ â”‚ Error        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚ Handling     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Route Handlers                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ /api/v1/     â”‚  â”‚ /api/v1/     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ tokenize     â”‚  â”‚ embed        â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ /api/v1/     â”‚  â”‚ /api/v1/     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ train        â”‚  â”‚ search       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ /api/v1/     â”‚  â”‚ /ws          â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ upload       â”‚  â”‚ (WebSocket)  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Business Logic Layer                      â”‚    â”‚
â”‚  â”‚  - TextTokenizer                                      â”‚    â”‚
â”‚  â”‚  - EmbeddingGenerator                                 â”‚    â”‚
â”‚  â”‚  - VectorStore                                        â”‚    â”‚
â”‚  â”‚  - JobManager (async)                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Response Layer                           â”‚    â”‚
â”‚  â”‚  - JSON responses                                    â”‚    â”‚
â”‚  â”‚  - Streaming responses                               â”‚    â”‚
â”‚  â”‚  - WebSocket messages                                â”‚    â”‚
â”‚  â”‚  - File downloads                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow

```
HTTP Request
    â†“
[FastAPI Router]
    Parse route â†’ Select handler
    â†“
[CORS Middleware]
    Add CORS headers
    â†“
[Authentication Middleware]
    Validate JWT token (if required)
    â†“
[Request Validation]
    Validate request body (Pydantic)
    â†“
[Route Handler]
    Execute business logic:
        - Tokenize text
        - Generate embeddings
        - Search vectors
        - etc.
    â†“
[Response Serialization]
    Convert to JSON
    â†“
[Response Middleware]
    Add headers, status codes
    â†“
HTTP Response
```

### WebSocket Flow

```
WebSocket Connection
    â†“
[Connection Handler]
    Accept connection
    â†“
[Message Loop]
    While connected:
        Receive message
            â†“
        [Message Router]
            Route to handler:
                - tokenize
                - train
                - stream
            â†“
        [Processing]
            Execute operation
            â†“
        [Streaming Response]
            Send progress updates
            â†“
        Send final result
    â†“
[Connection Close]
    Cleanup resources
```

### Job Management Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Async Job Management                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  POST /api/v1/jobs                                           â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Job Creation                                  â”‚    â”‚
â”‚  â”‚  - Generate job_id                                   â”‚    â”‚
â”‚  â”‚  - Create job record                                 â”‚    â”‚
â”‚  â”‚  - Status: PENDING                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Background Task                              â”‚    â”‚
â”‚  â”‚  - Execute in thread pool                            â”‚    â”‚
â”‚  â”‚  - Update status: RUNNING                            â”‚    â”‚
â”‚  â”‚  - Process request                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Status Updates                                â”‚    â”‚
â”‚  â”‚  - Update progress                                   â”‚    â”‚
â”‚  â”‚  - Store intermediate results                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Job Completion                                â”‚    â”‚
â”‚  â”‚  - Status: COMPLETED or FAILED                       â”‚    â”‚
â”‚  â”‚  - Store final results                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  GET /api/v1/jobs/{job_id}                                 â”‚
â”‚      â†“                                                       â”‚
â”‚  Return job status and results                              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoint Categories

**Core Endpoints:**
- `POST /api/v1/tokenize` - Tokenize text
- `POST /api/v1/embed` - Generate embeddings
- `POST /api/v1/analyze` - Comprehensive analysis

**Training Endpoints:**
- `POST /api/v1/train` - Train semantic model
- `GET /api/v1/training/jobs` - List training jobs
- `GET /api/v1/training/jobs/{id}` - Get job status

**File Operations:**
- `POST /api/v1/upload` - Upload file
- `POST /api/v1/tokenize/file` - Tokenize file
- `GET /api/v1/download/{id}` - Download results

**Search & Retrieval:**
- `POST /api/v1/search` - Vector search
- `GET /api/v1/health` - Health check
- `GET /api/v1/info` - System information

**WebSocket:**
- `WS /ws` - Real-time tokenization
- `WS /ws/train` - Training progress
- `WS /ws/execute` - Code execution

---

## ğŸ’¾ Vector Store Architecture - Clean Face

### Overview

SanTOK provides a unified interface to multiple vector database backends, allowing seamless switching between ChromaDB, FAISS, and Weaviate.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Vector Store Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Unified Interface                        â”‚    â”‚
â”‚  â”‚  SanTOKVectorStore (Abstract Base Class)             â”‚    â”‚
â”‚  â”‚  - add_tokens()                                      â”‚    â”‚
â”‚  â”‚  - search()                                          â”‚    â”‚
â”‚  â”‚  - get_token_embedding()                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Backend Selection                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ ChromaDB     â”‚  â”‚ FAISS        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ VectorStore  â”‚  â”‚ VectorStore   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Weaviate     â”‚  â”‚ In-Memory    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ VectorStore  â”‚  â”‚ VectorStore  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Backend-Specific Implementation          â”‚    â”‚
â”‚  â”‚  Each backend implements:                             â”‚    â”‚
â”‚  â”‚  - Storage mechanism                                  â”‚    â”‚
â”‚  â”‚  - Index structure                                    â”‚    â”‚
â”‚  â”‚  - Search algorithm                                   â”‚    â”‚
â”‚  â”‚  - Metadata handling                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ChromaDB Backend

```
ChromaVectorStore
    â†“
[Initialization]
    Create PersistentClient
    Get or create collection
    â†“
[Add Tokens]
    Convert embeddings to list
    Extract metadata from TokenRecords
    Add to collection with IDs
    â†“
[Search]
    Query collection with embedding
    Use similarity search
    Return top_k results with metadata
    â†“
[Retrieve]
    Get by ID from collection
    Return embedding vector
```

### FAISS Backend

```
FAISSVectorStore
    â†“
[Initialization]
    Create IndexFlatL2 (L2 distance)
    Initialize token mapping
    â†“
[Add Tokens]
    Add embeddings to FAISS index
    Store TokenRecord mapping
    â†“
[Search]
    Query FAISS index
    Get top_k indices
    Map indices to TokenRecords
    Return results with distances
    â†“
[Retrieve]
    Get embedding from index
    Return vector
```

### Weaviate Backend

```
WeaviateVectorStore
    â†“
[Initialization]
    Connect to Weaviate cluster
    Create or get class (collection)
    Define schema
    â†“
[Add Tokens]
    Create objects with:
        - Vector (embedding)
        - Properties (metadata)
    Batch insert
    â†“
[Search]
    Use GraphQL query
    Vector similarity search
    Filter by metadata (optional)
    Return results
    â†“
[Retrieve]
    Get object by ID
    Extract vector and metadata
    Return embedding
```

### Vector Store Comparison

| Feature | ChromaDB | FAISS | Weaviate |
|---------|----------|-------|----------|
| **Speed** | Fast | Very Fast | Fast |
| **Memory** | Medium | Low | Medium |
| **Persistence** | Built-in | Manual | Cloud |
| **Metadata** | Good | Limited | Excellent |
| **Scalability** | Medium | High | Very High |
| **Use Case** | Development | Production | Enterprise |

---

## ğŸ¤– Small Language Models (SLM) Architecture - Clean Face

### Overview

SanTOK includes a complete Small Language Model implementation that uses only SanTOK components - no external AI frameworks.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK SLM Architecture                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Input Layer                             â”‚    â”‚
â”‚  â”‚  - Text prompt                                      â”‚    â”‚
â”‚  â”‚  - Context (optional)                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Tokenization                            â”‚    â”‚
â”‚  â”‚  - SanTOK TextTokenizer                             â”‚    â”‚
â”‚  â”‚  - Convert text to TokenRecords                     â”‚    â”‚
â”‚  â”‚  - Extract UIDs                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Embedding Layer                         â”‚    â”‚
â”‚  â”‚  - SanTOK EmbeddingGenerator                        â”‚    â”‚
â”‚  â”‚  - Convert tokens to embeddings                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Model Architecture                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Transformer  â”‚â†’ â”‚ Attention    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Encoder      â”‚  â”‚ Mechanism    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Positional   â”‚â†’ â”‚ Feed-Forward â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Encoding     â”‚  â”‚ Network      â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Constraint Engine                        â”‚    â”‚
â”‚  â”‚  - Knowledge graph constraints                       â”‚    â”‚
â”‚  â”‚  - Fact validation                                  â”‚    â”‚
â”‚  â”‚  - No hallucination guarantee                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Decoder                                  â”‚    â”‚
â”‚  â”‚  - Constrained decoding                              â”‚    â”‚
â”‚  â”‚  - Token generation                                  â”‚    â”‚
â”‚  â”‚  - Sequence optimization                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Output                                  â”‚    â”‚
â”‚  â”‚  - Generated text                                   â”‚    â”‚
â”‚  â”‚  - Confidence scores                                â”‚    â”‚
â”‚  â”‚  - Reasoning trace                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Flow

```
Training Facts
    â†“
[Knowledge Integration]
    Add facts to UnifiedMemory
    Build knowledge graph
    â†“
[Tokenization]
    Tokenize all facts
    Build vocabulary
    â†“
[Embedding Training]
    Train semantic embeddings
    Learn token relationships
    â†“
[Model Training]
    Train transformer layers
    Learn sequence patterns
    â†“
[Constraint Learning]
    Learn graph constraints
    Build constraint rules
    â†“
Trained Model
```

### Generation Flow

```
Input Prompt
    â†“
[Tokenization]
    Tokenize prompt
    â†“
[Embedding]
    Convert to embeddings
    â†“
[Encoding]
    Pass through encoder
    Generate context
    â†“
[Constraint Checking]
    Query knowledge graph
    Get valid tokens
    â†“
[Decoding]
    Generate tokens one by one
    Apply constraints
    Optimize sequence
    â†“
[Output]
    Generated text
    Confidence scores
```

### Constraint-Grounded (CG-SLM) Features

- **No Hallucination**: Only generates facts from knowledge graph
- **Fact Validation**: Every token checked against constraints
- **Reasoning Trace**: Full explanation of generation
- **Confidence Scores**: Reliability for each token

---

## ğŸ’» CLI Architecture - Clean Face

### Overview

SanTOK provides comprehensive command-line interfaces for all operations, from tokenization to training to system management.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK CLI Architecture                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Command: python santok_cli.py <command> [options]          â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Argument Parser                          â”‚    â”‚
â”‚  â”‚  - Parse command-line arguments                      â”‚    â”‚
â”‚  â”‚  - Validate inputs                                   â”‚    â”‚
â”‚  â”‚  - Set defaults                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Command Router                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ tokenize     â”‚  â”‚ train        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Text       â”‚  â”‚ - Model      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - File       â”‚  â”‚ - Corpus     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - URL        â”‚  â”‚ - Enhanced   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ embed        â”‚  â”‚ test         â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Generate   â”‚  â”‚ - Quick      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Strategy   â”‚  â”‚ - Full       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚    â”‚
â”‚  â”‚  â”‚ info         â”‚                                   â”‚    â”‚
â”‚  â”‚  â”‚ - System     â”‚                                   â”‚    â”‚
â”‚  â”‚  â”‚ - Features   â”‚                                   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Execution Layer                          â”‚    â”‚
â”‚  â”‚  - Initialize components                             â”‚    â”‚
â”‚  â”‚  - Execute operation                                 â”‚    â”‚
â”‚  â”‚  - Handle errors                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Output Formatting                        â”‚    â”‚
â”‚  â”‚  - JSON output                                       â”‚    â”‚
â”‚  â”‚  - Pretty print                                      â”‚    â”‚
â”‚  â”‚  - File output                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CLI Command Structure

```
santok_cli.py
    â”œâ”€ tokenize
    â”‚   â”œâ”€ --text <text>
    â”‚   â”œâ”€ --file <path>
    â”‚   â”œâ”€ --url <url>
    â”‚   â”œâ”€ --method <method>
    â”‚   â”œâ”€ --output <path>
    â”‚   â””â”€ --format <json|txt>
    â”‚
    â”œâ”€ train
    â”‚   â”œâ”€ --file <corpus>
    â”‚   â”œâ”€ --model-path <path>
    â”‚   â”œâ”€ --embedding-dim <dim>
    â”‚   â”œâ”€ --epochs <n>
    â”‚   â””â”€ --enhanced
    â”‚
    â”œâ”€ embed
    â”‚   â”œâ”€ --text <text>
    â”‚   â”œâ”€ --model-path <path>
    â”‚   â”œâ”€ --strategy <strategy>
    â”‚   â””â”€ --output <path>
    â”‚
    â”œâ”€ test
    â”‚   â””â”€ --quick
    â”‚
    â””â”€ info
```

---

## ğŸ”— Integration Architecture - Clean Face

### Overview

SanTOK provides integration modules to connect with external systems, adapt vocabularies, and bridge between different components.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Integration Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Integration Modules                      â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Vocabulary   â”‚  â”‚ Source Map   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Adapter      â”‚  â”‚ Integration â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Convert    â”‚  â”‚ - Track      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   between    â”‚  â”‚   sources    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   systems    â”‚  â”‚ - Map tokens â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Cognitive    â”‚  â”‚ Vector       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Pipeline     â”‚  â”‚ Bridge       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - End-to-end â”‚  â”‚ - Connect    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   processing â”‚  â”‚   to stores  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Token        â”‚  â”‚ Embedding    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Bridge       â”‚  â”‚ Bridge       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Link       â”‚  â”‚ - Convert    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   systems    â”‚  â”‚   formats    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Integration Flow

```
External System
    â†“
[Integration Module]
    - Receive input
    - Convert format
    - Validate
    â†“
[SanTOK Processing]
    - Tokenize
    - Generate embeddings
    - Process
    â†“
[Output Conversion]
    - Convert to external format
    - Add metadata
    â†“
External System
```

---

## âš¡ Performance & Optimization Architecture - Clean Face

### Overview

SanTOK includes comprehensive performance optimization features including parallel processing, caching, and efficient algorithms.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Performance Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Performance Strategies                   â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Parallel     â”‚  â”‚ Caching      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Processing   â”‚  â”‚ System       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Threading  â”‚  â”‚ - Result     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Multiproc  â”‚  â”‚   caching    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Auto-detectâ”‚  â”‚ - Embedding  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   cache      â”‚                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”‚ Memory       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Optimization â”‚  â”‚ Algorithm   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚ Efficiency  â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Streaming  â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Chunking   â”‚  â”‚ - Sparse     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Lazy eval  â”‚  â”‚   matrices   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parallel Processing Flow

```
Input Text (Large)
    â†“
[Size Detection]
    Check text length
    â†“
[Threshold Check]
    If > 50KB:
        â†’ Use parallel processing
    Else:
        â†’ Use sequential processing
    â†“
[Chunking]
    Split text into chunks (50KB each)
    â†“
[Parallel Execution]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Chunk 1    â”‚  Chunk 2    â”‚  Chunk 3    â”‚
    â”‚  (Thread 1) â”‚  (Thread 2) â”‚  (Thread 3) â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚            â”‚            â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
            [Result Aggregation]
                Merge all results
                Maintain order
                â†“
            Final TokenStream
```

### Performance Optimization Strategies

**1. Automatic Parallel Processing:**
- Detects text size automatically
- Uses threading for I/O-bound tasks
- Uses multiprocessing for CPU-bound tasks
- Optimal worker count based on CPU cores

**2. Memory Optimization:**
- Streaming processing for large files
- Chunked processing to avoid memory overflow
- Lazy evaluation where possible
- Efficient data structures

**3. Caching:**
- Tokenization result caching
- Embedding caching
- Vocabulary caching
- Model caching

**4. Algorithm Efficiency:**
- Sparse matrices for large vocabularies
- Efficient hash-based lookups
- Optimized mathematical operations
- Vectorized operations (NumPy)

### Performance Benchmarks

```
Text Size    | Sequential | Threaded | Multiprocess | Speedup
-------------|------------|----------|--------------|--------
1 KB         | 0.001s     | 0.002s   | 0.005s       | 0.5x
10 KB        | 0.01s      | 0.008s   | 0.012s       | 1.25x
100 KB       | 0.1s       | 0.05s    | 0.04s        | 2.5x
1 MB         | 1.0s       | 0.3s     | 0.2s         | 5x
10 MB        | 10s        | 2s       | 1.5s         | 6.7x
```

---

## ğŸ›¡ï¸ Error Handling & Validation Architecture - Clean Face

### Overview

SanTOK implements comprehensive error handling and validation to ensure robust operation and prevent failures.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Error Handling Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Input/Request                                                â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Input Validation Layer                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Type         â”‚â†’ â”‚ Value        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚ Validation   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Range        â”‚â†’ â”‚ Format       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚ Validation   â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Processing Layer                        â”‚    â”‚
â”‚  â”‚  - Try-catch blocks                                 â”‚    â”‚
â”‚  â”‚  - Graceful degradation                             â”‚    â”‚
â”‚  â”‚  - Fallback mechanisms                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Error Classification                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚ Processing   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Errors       â”‚  â”‚ Errors       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ System       â”‚  â”‚ External     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Errors       â”‚  â”‚ Errors       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Error Response                          â”‚    â”‚
â”‚  â”‚  - User-friendly messages                           â”‚    â”‚
â”‚  â”‚  - Detailed logs (server-side)                      â”‚    â”‚
â”‚  â”‚  - Error codes                                      â”‚    â”‚
â”‚  â”‚  - Recovery suggestions                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Validation Layers

**1. Input Validation:**
```python
validate_text_input(text)      # Type and format check
validate_port(port)             # Range validation
validate_file_path(path)        # Path validation
validate_tokenization_method()  # Method validation
```

**2. Processing Validation:**
- Token count limits
- Memory usage checks
- Timeout handling
- Resource availability

**3. Output Validation:**
- Result format validation
- Data integrity checks
- Consistency verification

### Error Handling Strategy

```
Error Occurs
    â†“
[Error Classification]
    - ValidationError
    - ProcessingError
    - SystemError
    - ExternalError
    â†“
[Error Context]
    - Capture stack trace
    - Log context
    - Identify recovery options
    â†“
[Error Response]
    Production:
        - Generic user message
        - Detailed server logs
    Development:
        - Detailed error message
        - Stack trace
        - Debug information
    â†“
[Recovery Attempt]
    - Fallback methods
    - Retry logic
    - Graceful degradation
```

### Security Considerations

- **Information Disclosure Prevention**: Detailed errors only in development
- **Input Sanitization**: All inputs validated and sanitized
- **Resource Limits**: Prevent DoS attacks
- **Authentication**: JWT-based security
- **CORS Configuration**: Configurable origins

---

## ğŸ”„ Complete Data Flow Architecture - Clean Face

### Overview

This section shows the complete end-to-end data flow through the entire SanTOK system.

### End-to-End Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Complete SanTOK Data Flow                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [1] Input Sources                                   â”‚    â”‚
â”‚  â”‚  - Text string                                       â”‚    â”‚
â”‚  â”‚  - File upload                                       â”‚    â”‚
â”‚  â”‚  - URL fetch                                         â”‚    â”‚
â”‚  â”‚  - API request                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [2] Preprocessing                                    â”‚    â”‚
â”‚  â”‚  - Text normalization                                â”‚    â”‚
â”‚  â”‚  - Language detection                                â”‚    â”‚
â”‚  â”‚  - Encoding detection                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [3] Tokenization (9 methods in parallel)            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚Space â”‚ â”‚Word  â”‚ â”‚Char  â”‚ â”‚Gram  â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚Subw  â”‚ â”‚BPE   â”‚ â”‚Syll  â”‚ â”‚Freq  â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”                                          â”‚    â”‚
â”‚  â”‚  â”‚Byte  â”‚                                          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [4] Mathematical Enrichment                         â”‚    â”‚
â”‚  â”‚  - UID assignment                                    â”‚    â”‚
â”‚  â”‚  - Frontend digits                                   â”‚    â”‚
â”‚  â”‚  - Backend numbers                                   â”‚    â”‚
â”‚  â”‚  - Global IDs                                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [5] Branch Point                                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Embedding    â”‚  â”‚ Cognitive    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Path         â”‚  â”‚ Reasoning    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚ Path         â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚         â”‚                 â”‚                         â”‚    â”‚
â”‚  â”‚         â†“                 â†“                         â”‚    â”‚
â”‚  â”‚  [5a] Embedding    [5b] Knowledge                  â”‚    â”‚
â”‚  â”‚  Generation        Graph Building                  â”‚    â”‚
â”‚  â”‚         â”‚                 â”‚                         â”‚    â”‚
â”‚  â”‚         â†“                 â†“                         â”‚    â”‚
â”‚  â”‚  [5c] Vector Store  [5d] Reasoning                 â”‚    â”‚
â”‚  â”‚         â”‚                 â”‚                         â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚    â”‚
â”‚  â”‚                  â†“                                  â”‚    â”‚
â”‚  â”‚         [6] Integration                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [7] Output Generation                                â”‚    â”‚
â”‚  â”‚  - Formatted results                                  â”‚    â”‚
â”‚  â”‚  - Metadata                                           â”‚    â”‚
â”‚  â”‚  - Explanations                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [8] Response Delivery                                â”‚    â”‚
â”‚  â”‚  - JSON response                                      â”‚    â”‚
â”‚  â”‚  - File download                                      â”‚    â”‚
â”‚  â”‚  - WebSocket stream                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Processing Pipeline

```
1. INPUT
   Text: "Hello World"
   Source: API/File/URL
   â†“
2. PREPROCESSING
   normalize_case() â†’ "hello world"
   normalize_whitespace() â†’ "hello world"
   detect_language() â†’ "en"
   â†“
3. TOKENIZATION (Parallel)
   Space: ["hello", "world"]
   Word: ["hello", "world"]
   Char: ["h", "e", "l", ...]
   Grammar: ["hello", ",", "world"]
   Subword: ["hel", "lo", "wor", "ld"]
   ... (9 methods)
   â†“
4. MATHEMATICAL ANALYSIS
   For each token:
     - Generate UID (Xorshift64*)
     - Calculate frontend digit (9-centric)
     - Compose backend number
     - Assign global ID
     - Link neighbors
   â†“
5. EMBEDDING GENERATION
   Strategy: feature_based/semantic/hash/hybrid
   Extract features â†’ Generate vector (768-dim)
   â†“
6. STORAGE/REASONING
   Option A: Vector Store
     - Add to ChromaDB/FAISS/Weaviate
     - Index for search
   Option B: Cognitive Reasoning
     - Add to knowledge graph
     - Build relations
     - Enable reasoning
   â†“
7. OUTPUT FORMATTING
   {
     "tokens": [...],
     "embeddings": [...],
     "metadata": {...},
     "reasoning": {...}
   }
   â†“
8. RESPONSE
   JSON/File/Stream
```

---

## ğŸš€ Deployment Architecture - Clean Face

### Overview

SanTOK supports multiple deployment scenarios from local development to cloud production.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Deployment Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Deployment Options                       â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Local        â”‚  â”‚ Cloud        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Development  â”‚  â”‚ Production   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Python     â”‚  â”‚ - Railway    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   script     â”‚  â”‚ - Heroku     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - CLI        â”‚  â”‚ - AWS        â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Docker       â”‚  â”‚ Kubernetes   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Container    â”‚  â”‚ Cluster      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - docker-composeâ”‚ - Auto-scalingâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Dockerfile â”‚  â”‚ - Load bal.  â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Scenarios

**1. Local Development:**
```
python run.py
    â†“
[Development Server]
    - Hot reload
    - Debug mode
    - Local storage
    - Port: 8000
```

**2. Production (Railway/Heroku):**
```
Procfile: web: python start.py
    â†“
[Platform Detection]
    - Auto-detect Python
    - Set PORT from env
    - Configure logging
    â†“
[Production Server]
    - Optimized settings
    - Error handling
    - Logging
    - Health checks
```

**3. Docker Deployment:**
```
docker-compose up
    â†“
[Docker Container]
    - Isolated environment
    - Volume mounts
    - Network config
    - Environment variables
```

**4. Kubernetes:**
```
kubectl apply -f k8s/
    â†“
[K8s Cluster]
    - Pods
    - Services
    - Ingress
    - Auto-scaling
```

### Environment Configuration

```
Development:
    - DEBUG=True
    - LOG_LEVEL=DEBUG
    - CORS_ORIGINS=*
    - PORT=8000

Production:
    - DEBUG=False
    - LOG_LEVEL=INFO
    - CORS_ORIGINS=https://yourdomain.com
    - PORT=${PORT}
    - WEAVIATE_URL=${WEAVIATE_URL}
    - WEAVIATE_API_KEY=${WEAVIATE_API_KEY}
```

---

## ğŸ” Security Architecture - Clean Face

### Overview

SanTOK implements multiple security layers to protect the system and user data.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Security Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Security Layers                         â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Input        â”‚â†’ â”‚ Authenticationâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚ (JWT)        â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Sanitization â”‚â†’ â”‚ Authorizationâ”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚ (Roles)      â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Rate         â”‚â†’ â”‚ Error        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Limiting     â”‚  â”‚ Masking      â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Security Features

**1. Authentication:**
- JWT-based token authentication
- Token expiration
- Secure token storage
- Refresh tokens

**2. Input Validation:**
- Type checking
- Range validation
- Format validation
- Sanitization

**3. Resource Protection:**
- Rate limiting
- File size limits
- Memory limits
- Timeout protection

**4. Error Security:**
- No information disclosure in production
- Detailed errors only in development
- Secure logging
- Error masking

**5. CORS Configuration:**
- Configurable origins
- Production restrictions
- Development flexibility

---

## ğŸ“Š Monitoring & Logging Architecture - Clean Face

### Overview

SanTOK includes comprehensive monitoring and logging for observability and debugging.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Monitoring Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Logging System                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Application  â”‚  â”‚ Error        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Logs         â”‚  â”‚ Logs         â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Performance  â”‚  â”‚ Access       â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Logs         â”‚  â”‚ Logs         â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Metrics Collection                       â”‚    â”‚
â”‚  â”‚  - Request count                                     â”‚    â”‚
â”‚  â”‚  - Response times                                    â”‚    â”‚
â”‚  â”‚  - Error rates                                       â”‚    â”‚
â”‚  â”‚  - Resource usage                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Logging Levels

- **DEBUG**: Detailed debugging information
- **INFO**: General informational messages
- **WARNING**: Warning messages
- **ERROR**: Error messages
- **CRITICAL**: Critical errors

### Health Check Endpoints

- `GET /api/v1/health` - Basic health check
- `GET /api/v1/info` - System information
- `GET /api/v1/metrics` - Performance metrics

---

## ğŸ—œï¸ Compression Architecture - Clean Face

### Overview

SanTOK includes text compression algorithms based on mathematical properties and 9-centric numerology.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Compression Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Input Text                                                   â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Compression Strategies                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Numerology   â”‚  â”‚ Weighted     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Based        â”‚  â”‚ Sum Based    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Digital Root â”‚  â”‚ Backend      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Folding      â”‚  â”‚ Number       â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Compression Process                           â”‚    â”‚
â”‚  â”‚  1. Calculate numerology values                       â”‚    â”‚
â”‚  â”‚  2. Compute weighted character sums                   â”‚    â”‚
â”‚  â”‚  3. Apply digital root folding                        â”‚    â”‚
â”‚  â”‚  4. Generate compressed representation               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  Compressed Output                                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compression Algorithms

**1. Numerology-Based Compression:**
```
Text: "Hello"
    â†“
[Character Numerology]
    H â†’ 8 (position in alphabet % 9 + 1)
    e â†’ 5
    l â†’ 3
    l â†’ 3
    o â†’ 6
    â†“
[Sum Calculation]
    Total: 8 + 5 + 3 + 3 + 6 = 25
    â†“
[Digital Root]
    dr(25) = 1 + ((25-1) mod 9) = 7
    â†“
Compressed: 7
```

**2. Weighted Sum Compression:**
```
Text: "Hello"
    â†“
[Weighted Sum]
    H: ord('H') Ã— 1 = 72 Ã— 1 = 72
    e: ord('e') Ã— 2 = 101 Ã— 2 = 202
    l: ord('l') Ã— 3 = 108 Ã— 3 = 324
    l: ord('l') Ã— 4 = 108 Ã— 4 = 432
    o: ord('o') Ã— 5 = 111 Ã— 5 = 555
    â†“
Total: 72 + 202 + 324 + 432 + 555 = 1585
    â†“
[Digital Root]
    dr(1585) = 1 + ((1585-1) mod 9) = 1
    â†“
Compressed: 1
```

---

## ğŸ—ºï¸ Source Map Integration Architecture - Clean Face

### Overview

SanTOK's source map integration tracks the provenance of tokens and embeddings, enabling source-aware processing and multi-source merging.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Source Map Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Source Map Structure                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Source       â”‚  â”‚ Algorithm    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Metadata     â”‚  â”‚ Mapping      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - source_id  â”‚  â”‚ - algorithm  â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - source_tag â”‚  â”‚   â†’ tokens   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - timestamp  â”‚  â”‚ - tokens â†’   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - metadata   â”‚  â”‚   source     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Source-Aware Tokenization                     â”‚    â”‚
â”‚  â”‚  - Tag tokens with source                            â”‚    â”‚
â”‚  â”‚  - Track algorithm used                              â”‚    â”‚
â”‚  â”‚  - Maintain provenance                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Source-Aware Embedding Generation             â”‚    â”‚
â”‚  â”‚  - Embeddings linked to source                       â”‚    â”‚
â”‚  â”‚  - Source metadata in embeddings                     â”‚    â”‚
â”‚  â”‚  - Multi-source merging                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Source Map Flow

```
Input Text + Source Tag
    â†“
[Source Map Lookup]
    Get source metadata:
        - source_id
        - source_tag (e.g., "wikipedia", "arxiv")
        - algorithm_id
    â†“
[Tokenization with Source]
    Tokenize text
    Tag each token with:
        - source_id
        - source_tag
        - algorithm_id
    â†“
[Embedding Generation with Source]
    Generate embeddings
    Link embeddings to source
    Add source metadata
    â†“
[Source-Aware Storage]
    Store with source tags
    Enable source-based queries
    â†“
[Multi-Source Merging]
    Merge embeddings from multiple sources
    Combine metadata
    Weighted combination
```

---

## ğŸ§© Data Interpretation Architecture - Clean Face

### Overview

SanTOK's data interpretation system uses embeddings and vector stores to provide real-time insights and interpretations of data.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Data Interpretation Architecture            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Input Text: "Sales dropped 20% last month"                   â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Token Extraction                              â”‚    â”‚
â”‚  â”‚  - Tokenize input                                     â”‚    â”‚
â”‚  â”‚  - Extract key tokens                                â”‚    â”‚
â”‚  â”‚  - Identify important terms                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Embedding Generation                          â”‚    â”‚
â”‚  â”‚  - Generate embeddings for tokens                    â”‚    â”‚
â”‚  â”‚  - Create query embedding                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Vector Search                                 â”‚    â”‚
â”‚  â”‚  - Search in Weaviate/ChromaDB/FAISS                â”‚    â”‚
â”‚  â”‚  - Find related concepts                            â”‚    â”‚
â”‚  â”‚  - Retrieve top-k results                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Interpretation Generation                     â”‚    â”‚
â”‚  â”‚  - Analyze relationships                             â”‚    â”‚
â”‚  â”‚  - Generate insights                                â”‚    â”‚
â”‚  â”‚  - Provide recommendations                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  Output: "Analyze customer behavior and marketing changes"   â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Interpretation Flow

```
Input: "Sales dropped 20% last month"
    â†“
[Token Extraction]
    Key tokens: ["Sales", "dropped", "20%", "last", "month"]
    â†“
[Embedding Generation]
    Generate embeddings for each token
    Create combined query embedding
    â†“
[Vector Search]
    Search in knowledge base:
        - Find "Sales" related concepts
        - Find "dropped" related concepts
        - Find "20%" related concepts
    â†“
[Concept Retrieval]
    Related concepts:
        - "customer behavior"
        - "marketing changes"
        - "trend analysis"
        - "improvement strategies"
    â†“
[Interpretation Generation]
    Combine concepts:
        "Analyze customer behavior and 
         marketing changes to find the cause."
    â†“
Output with confidence scores
```

---

## ğŸ¯ Custom Algorithms Architecture - Clean Face

### Overview

SanTOK includes several custom algorithms for ranking, scoring, similarity, and graph operations.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Custom Algorithms Architecture              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Algorithm Categories                          â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Ranking      â”‚  â”‚ Scoring      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Algorithms   â”‚  â”‚ Algorithms   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - SanTOK     â”‚  â”‚ - 9-Scorer   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   Ranker     â”‚  â”‚ - Confidence â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Similarity   â”‚  â”‚ Graph        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Algorithms   â”‚  â”‚ Algorithms   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Semantic   â”‚  â”‚ - Graph      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   Similarity â”‚  â”‚   Walker     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Pattern      â”‚  â”‚ Query        â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Matching     â”‚  â”‚ Parsing      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Pattern    â”‚  â”‚ - NL â†’ Query â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   Matcher    â”‚  â”‚   Parser     â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SanTOK Ranker Architecture

```
Query + Candidates
    â†“
[Component Score Calculation]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Relevance    â”‚ Connectivity â”‚ Hierarchy    â”‚ Freshness    â”‚
    â”‚              â”‚              â”‚              â”‚              â”‚
    â”‚ - Token      â”‚ - Graph      â”‚ - Tree       â”‚ - Temporal   â”‚
    â”‚   overlap    â”‚   centrality â”‚   depth      â”‚   decay      â”‚
    â”‚ - Position   â”‚ - Relation   â”‚ - Sibling    â”‚ - Access     â”‚
    â”‚   boost      â”‚   strength   â”‚   penalty    â”‚   frequency  â”‚
    â”‚ - Digital    â”‚ - Path       â”‚ - Parent     â”‚ - Mod time   â”‚
    â”‚   root       â”‚   distance   â”‚   inheritanceâ”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Weighted Combination]
    score = Î±Â·Relevance + Î²Â·Connectivity + Î³Â·Hierarchy + Î´Â·Freshness
    (default: Î±=0.4, Î²=0.3, Î³=0.2, Î´=0.1)
    â†“
[9-Centric Folding]
    Apply digital root transformation
    â†“
Ranked Results
```

### 9-Scorer Architecture

```
Input Value
    â†“
[9-Centric Calculation]
    Apply digital root: dr(n) = 1 + ((n-1) mod 9)
    â†“
[Score Normalization]
    Map to [0, 1] range
    â†“
[Confidence Score]
    Final score (0-1)
```

### Semantic Similarity Architecture

```
Token A + Token B
    â†“
[Feature Extraction]
    Extract features from both tokens
    â†“
[Similarity Calculation]
    - Character overlap
    - UID distance
    - Frontend digit similarity
    - Backend number proximity
    â†“
[Combined Similarity]
    Weighted combination of features
    â†“
Similarity Score (0-1)
```

### Graph Walker Architecture

```
Start Node
    â†“
[Energy-Based Traversal]
    - Calculate node energy
    - Follow high-energy paths
    - Avoid low-energy nodes
    â†“
[Path Exploration]
    - BFS/DFS traversal
    - Depth limits
    - Energy thresholds
    â†“
[Path Ranking]
    Rank paths by:
        - Total energy
        - Path length
        - Relation strength
    â†“
Top-K Paths
```

### Pattern Matcher Architecture

```
Input Text
    â†“
[Pattern Library]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Lexical      â”‚ Structural   â”‚ Copula       â”‚ Possessive   â”‚
    â”‚ Patterns     â”‚ Patterns     â”‚ Patterns     â”‚ Patterns     â”‚
    â”‚              â”‚              â”‚              â”‚              â”‚
    â”‚ - IS_A       â”‚ - Position   â”‚ - "X is Y"   â”‚ - "X's Y"    â”‚
    â”‚ - PART_OF    â”‚ - Distance   â”‚ - "X are Y"  â”‚ - "Y of X"   â”‚
    â”‚ - HAS_PART   â”‚ - Context    â”‚              â”‚              â”‚
    â”‚ - CAUSES     â”‚              â”‚              â”‚              â”‚
    â”‚ - USES       â”‚              â”‚              â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Pattern Matching]
    - Apply regex patterns
    - Extract subject/object
    - Identify relation type
    - Calculate confidence
    â†“
[9-Centric Scoring]
    Apply digital root to confidence
    â†“
[Relation Extraction]
    Output: (subject, relation, object, confidence)
```

**Pattern Types:**
- **Lexical Patterns**: Word-based regex patterns (e.g., "X is Y" â†’ IS_A)
- **Structural Patterns**: Position-based extraction
- **Copula Patterns**: "X is Y", "X are Y" â†’ IS_A relation
- **Possessive Patterns**: "X's Y", "Y of X" â†’ PART_OF/HAS_PART
- **Causal Patterns**: "X causes Y", "because of X" â†’ CAUSES
- **Temporal Patterns**: "X before Y", "after X" â†’ TEMPORAL

**Example:**
```python
matcher = SanTOKPatternMatcher()
text = "Python is a programming language. It uses dynamic typing."
matches = matcher.extract(text)
# Output: [
#   (Python, IS_A, programming language, 0.9),
#   (Python, USES, dynamic typing, 0.8)
# ]
```

### Query Parser Architecture

```
Natural Language Query
    â†“
[Query Type Detection]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Definition   â”‚ Relation     â”‚ List         â”‚ Boolean      â”‚
    â”‚ "What is X?" â”‚ "How Xâ†’Y?"   â”‚ "Parts of X" â”‚ "Is X a Y?"  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Comparison   â”‚ Process      â”‚ Count        â”‚
    â”‚ "X vs Y?"    â”‚ "How X works"â”‚ "How many X?"â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Entity Extraction]
    - Extract subject
    - Extract object (if relation query)
    - Extract modifiers (negation, quantifiers)
    â†“
[Query Structure]
    {
        "type": "definition|relation|list|...",
        "subject": "extracted entity",
        "object": "extracted entity (optional)",
        "negated": false,
        "quantifier": null,
        "confidence": 0.95
    }
    â†“
[Structured Query]
    Ready for execution against knowledge base
```

**Supported Query Types:**
- **Definition**: "What is X?", "Define X", "Tell me about X"
- **Relation**: "How is X related to Y?", "What's the relationship between X and Y?"
- **List**: "What are the parts of X?", "List all X", "What does X contain?"
- **Boolean**: "Is X a Y?", "Does X have Y?"
- **Comparison**: "What's the difference between X and Y?", "Compare X and Y"
- **Process**: "How does X work?", "Explain how X operates"
- **Count**: "How many X?", "Count the number of Y"
- **Cause**: "Why does X happen?", "What causes Y?"

**Example:**
```python
parser = SanTOKQueryParser()
query = parser.parse("What is machine learning?")
# Output:
#   type: DEFINITION
#   subject: "machine learning"
#   confidence: 0.95
```

### Semantic Similarity Architecture (Detailed)

```
Text A + Text B
    â†“
[Tokenization]
    Tokenize both texts
    â†“
[Multi-Component Analysis]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component 1: Lexical Similarity                        â”‚
    â”‚   - Jaccard coefficient (token overlap)                â”‚
    â”‚   - Dice coefficient                                    â”‚
    â”‚   - Common tokens identification                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component 2: N-gram Similarity                          â”‚
    â”‚   - Character n-gram extraction (default: trigrams)     â”‚
    â”‚   - N-gram overlap calculation                          â”‚
    â”‚   - Position-aware matching                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component 3: Position-Weighted Similarity              â”‚
    â”‚   - Token position matching                             â”‚
    â”‚   - Order preservation scoring                          â”‚
    â”‚   - Distance-based weighting                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component 4: Graph-Based Similarity (Optional)          â”‚
    â”‚   - Path distance in knowledge graph                    â”‚
    â”‚   - Relation strength                                   â”‚
    â”‚   - Common neighbors                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Weighted Combination]
    score = Î±Â·Lexical + Î²Â·Ngram + Î³Â·Position + Î´Â·Graph
    (default: Î±=0.35, Î²=0.25, Î³=0.20, Î´=0.20)
    â†“
[9-Centric Harmonization]
    Apply digital root transformation
    Normalize to [0, 1] range
    â†“
SimilarityResult
    - Combined score (0-1)
    - Digital root (1-9)
    - Component breakdown
    - Common tokens/ngrams
```

**Similarity Formula:**
```
sim(a, b) = Î±Â·Jaccard(a, b) + Î²Â·Ngram(a, b) + Î³Â·Position(a, b) + Î´Â·Graph(a, b)

Where:
- Jaccard(a, b) = |A âˆ© B| / |A âˆª B|
- Ngram(a, b) = |N-grams(a) âˆ© N-grams(b)| / |N-grams(a) âˆª N-grams(b)|
- Position(a, b) = Weighted position alignment score
- Graph(a, b) = Path distance / max_path_distance (if graph available)
```

**Example:**
```python
similarity = SanTOKSimilarity(graph=knowledge_graph)
result = similarity.compute("machine learning", "deep learning")
# Output:
#   score: 0.67
#   digital_root: 4
#   lexical_score: 0.60
#   ngram_score: 0.75
#   position_score: 0.55
#   graph_score: 0.70
```

---

## âš™ï¸ Configuration & Utilities Architecture - Clean Face

### Overview

SanTOK includes comprehensive configuration management and utility systems for system-wide settings and operations.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Configuration Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Configuration Sources                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Environment  â”‚  â”‚ Config Files â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Variables     â”‚  â”‚ (.env, yaml) â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Default      â”‚  â”‚ Runtime      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Values       â”‚  â”‚ Overrides    â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Configuration Manager                        â”‚    â”‚
â”‚  â”‚  - Load configurations                              â”‚    â”‚
â”‚  â”‚  - Merge sources                                    â”‚    â”‚
â”‚  â”‚  - Validate settings                                â”‚    â”‚
â”‚  â”‚  - Provide defaults                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Configuration Categories                     â”‚    â”‚
â”‚  â”‚  - Server settings (port, host, CORS)               â”‚    â”‚
â”‚  â”‚  - Tokenization settings (seed, methods)            â”‚    â”‚
â”‚  â”‚  - Embedding settings (dim, strategy)               â”‚    â”‚
â”‚  â”‚  - Vector store settings (backend, connection)       â”‚    â”‚
â”‚  â”‚  - Logging settings (level, format)                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Hierarchy

```
1. Runtime Overrides (highest priority)
    - Command-line arguments
    - Function parameters
    â†“
2. Environment Variables
    - PORT, LOG_LEVEL, etc.
    â†“
3. Config Files
    - .env file
    - config.yaml
    â†“
4. Default Values (lowest priority)
    - Built-in defaults
```

### Utility Systems

**1. Validation Utilities:**
- Input type validation
- Range validation
- Format validation
- Path validation

**2. Logging Utilities:**
- Structured logging
- Log levels
- File/console output
- Log rotation

**3. Unique Identifier Utilities:**
- UID generation (Xorshift64*)
- ID management
- Collision detection

**4. Formatting Utilities:**
- Output formatting
- Data serialization
- Pretty printing

---

## ğŸ”„ Memory Management Architecture - Clean Face

### Overview

SanTOK implements efficient memory management for handling large datasets and long-running processes.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SanTOK Memory Management Architecture              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Memory Strategies                             â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Streaming    â”‚  â”‚ Chunking     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Processing   â”‚  â”‚ Strategy     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Process    â”‚  â”‚ - Split into â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   in chunks  â”‚  â”‚   chunks     â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Release    â”‚  â”‚ - Process    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   memory     â”‚  â”‚   separately â”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚
â”‚  â”‚  â”‚ Caching      â”‚  â”‚ Lazy         â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ Strategy     â”‚  â”‚ Evaluation   â”‚                â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Result     â”‚  â”‚ - Compute    â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   caching    â”‚  â”‚   on demand  â”‚                â”‚    â”‚
â”‚  â”‚  â”‚ - Embedding  â”‚  â”‚ - Defer      â”‚                â”‚    â”‚
â”‚  â”‚  â”‚   caching    â”‚  â”‚   computationâ”‚                â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Optimization Techniques

**1. Streaming Processing:**
- Process data in streams
- Release memory after each chunk
- Avoid loading entire dataset

**2. Chunking:**
- Split large texts into chunks
- Process chunks independently
- Aggregate results

**3. Caching:**
- Cache frequently used results
- LRU eviction policy
- Memory-bounded cache

**4. Lazy Evaluation:**
- Compute only when needed
- Defer expensive operations
- Generator-based processing

---

## ğŸ“ Detailed Component Architectures

### 1. SanTOK Core Tokenization Architecture

**Core Tokenization Engine** - The foundation of all text processing.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Core Tokenization Engine                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Input Text                                                  â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Preprocessing Layer                â”‚                   â”‚
â”‚  â”‚  - Case normalization                 â”‚                   â”‚
â”‚  â”‚  - Punctuation handling              â”‚                   â”‚
â”‚  â”‚  - Whitespace normalization          â”‚                   â”‚
â”‚  â”‚  - Language detection                â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Tokenization Methods (9 types)    â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”       â”‚                   â”‚
â”‚  â”‚  â”‚Space â”‚ â”‚ Word â”‚ â”‚ Char â”‚       â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜       â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”       â”‚                   â”‚
â”‚  â”‚  â”‚Grammarâ”‚ â”‚Subwordâ”‚ â”‚ Byte â”‚       â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜       â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”       â”‚                   â”‚
â”‚  â”‚  â”‚BPE   â”‚ â”‚Syllableâ”‚ â”‚Freq  â”‚       â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜       â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Mathematical Analysis Layer        â”‚                   â”‚
â”‚  â”‚  - UID Generation (Xorshift64*)      â”‚                   â”‚
â”‚  â”‚  - Frontend Digit Calculation        â”‚                   â”‚
â”‚  â”‚  - Backend Number Composition       â”‚                   â”‚
â”‚  â”‚  - Global ID Assignment              â”‚                   â”‚
â”‚  â”‚  - Digital Root Computation          â”‚                   â”‚
â”‚  â”‚  - Neighbor UID Linking              â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Statistical Features              â”‚                   â”‚
â”‚  â”‚  - Length Factor                    â”‚                   â”‚
â”‚  â”‚  - Balance Index                    â”‚                   â”‚
â”‚  â”‚  - Entropy Index                    â”‚                   â”‚
â”‚  â”‚  - Mean & Variance                 â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  TokenStream Objects (with TokenRecord instances)          â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Token Processing Pipeline:**

```
Text Input
    â†“
[Preprocessing]
    â”œâ”€ normalize_case()
    â”œâ”€ remove_punctuation()
    â”œâ”€ normalize_whitespace()
    â””â”€ detect_language()
    â†“
[Tokenization] (Parallel execution for 9 methods)
    â”œâ”€ tokenize_space()      â†’ Space tokens
    â”œâ”€ tokenize_word()       â†’ Word tokens
    â”œâ”€ tokenize_char()       â†’ Character tokens
    â”œâ”€ tokenize_grammar()    â†’ Grammar tokens
    â”œâ”€ tokenize_subword()    â†’ Subword tokens
    â”œâ”€ tokenize_subword_bpe() â†’ BPE tokens
    â”œâ”€ tokenize_subword_syllable() â†’ Syllable tokens
    â”œâ”€ tokenize_subword_frequency() â†’ Frequency tokens
    â””â”€ tokenize_bytes()      â†’ Byte tokens
    â†“
[UID Assignment]
    â”œâ”€ assign_uids(seed)     â†’ Xorshift64* based UIDs
    â””â”€ neighbor_uids()        â†’ Link prev/next UIDs
    â†“
[Mathematical Properties]
    â”œâ”€ frontend_digit         â†’ 9-centric digit (1-9)
    â”œâ”€ backend_number         â†’ Composite number
    â”œâ”€ global_id              â†’ Unique global identifier
    â””â”€ content_id             â†’ Content-based ID
    â†“
[TokenStream Creation]
    â””â”€ TokenStream with TokenRecord objects
```

**Key Classes:**
- `TextTokenizer` - Main orchestrator
- `TokenStream` - Container for tokenized results
- `TokenRecord` - Individual token with all properties

---

### 2. SanTOK Cognitive Architecture

**Cognitive Reasoning System** - Deterministic reasoning substrate.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Cognitive System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Knowledge Storage Layer                 â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚  â”‚  â”‚  GraphStore   â”‚      â”‚  TreeStore    â”‚            â”‚    â”‚
â”‚  â”‚  â”‚              â”‚      â”‚              â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - Nodes      â”‚      â”‚ - Root Nodes â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - Edges      â”‚      â”‚ - Children   â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - Relations  â”‚      â”‚ - Hierarchy  â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ (15+ types)  â”‚      â”‚ - Taxonomies â”‚            â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚  â”‚         â”‚                      â”‚                     â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚  â”‚                    â†“                                 â”‚    â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚    â”‚
â”‚  â”‚         â”‚  UnifiedMemory       â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  - MemoryObjects     â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  - Graph linking     â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  - Auto-relations   â”‚                    â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Reasoning Engine                          â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚  â”‚  â”‚ Inference     â”‚      â”‚ Query Engine  â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ Engine        â”‚      â”‚              â”‚            â”‚    â”‚
â”‚  â”‚  â”‚              â”‚      â”‚ - Parsing    â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - 20+ Rules  â”‚      â”‚ - Execution  â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - Chaining   â”‚      â”‚ - Results    â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ - Validation â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚                     â”‚    â”‚
â”‚  â”‚         â”‚                     â”‚                     â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚  â”‚                    â†“                                 â”‚    â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚    â”‚
â”‚  â”‚         â”‚  PathFinder          â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  - Graph traversal  â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  - Path discovery    â”‚                    â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚
â”‚  â”‚                    â†“                                 â”‚    â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚    â”‚
â”‚  â”‚         â”‚  Contradiction       â”‚                    â”‚    â”‚
â”‚  â”‚         â”‚  Detector            â”‚                    â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Explanation Layer                       â”‚    â”‚
â”‚  â”‚  - Reasoning traces                                  â”‚    â”‚
â”‚  â”‚  - Confidence scores                                 â”‚    â”‚
â”‚  â”‚  - Source attribution                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Knowledge Graph Structure:**

```
GraphNode
    â”œâ”€ id: str
    â”œâ”€ label: str
    â”œâ”€ properties: dict
    â””â”€ edges: List[GraphEdge]

GraphEdge
    â”œâ”€ source: str (node id)
    â”œâ”€ target: str (node id)
    â”œâ”€ relation: RelationType (15+ types)
    â”‚   â”œâ”€ IS_A
    â”‚   â”œâ”€ PART_OF
    â”‚   â”œâ”€ CAUSES
    â”‚   â”œâ”€ USES
    â”‚   â”œâ”€ LOCATED_IN
    â”‚   â””â”€ ... (10+ more)
    â””â”€ confidence: float (0-1)
```

**Reasoning Flow:**

```
Query Input
    â†“
[Query Parser]
    â””â”€ Parse natural language â†’ Structured query
    â†“
[Query Engine]
    â”œâ”€ Find relevant nodes in graph
    â”œâ”€ Extract relations
    â””â”€ Build query plan
    â†“
[Inference Engine]
    â”œâ”€ Apply inference rules (20+)
    â”‚   â”œâ”€ Transitivity
    â”‚   â”œâ”€ Inheritance
    â”‚   â”œâ”€ Symmetry
    â”‚   â”œâ”€ Inverse
    â”‚   â””â”€ ... (16+ more)
    â”œâ”€ Rule chaining
    â””â”€ Confidence propagation
    â†“
[Path Finder]
    â””â”€ Find reasoning paths
    â†“
[Contradiction Detector]
    â””â”€ Validate consistency
    â†“
[Explainer]
    â”œâ”€ Generate reasoning trace
    â”œâ”€ Calculate confidence
    â””â”€ Format explanation
    â†“
Answer with full trace
```

---

### 3. Embedding System Architecture

**Semantic Embedding Generation** - Multiple strategies for vector generation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Embedding System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  TokenRecord Input                                            â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Strategy Selection                â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                   â”‚
â”‚  â”‚  â”‚ Feature  â”‚  â”‚ Semantic â”‚       â”‚                   â”‚
â”‚  â”‚  â”‚ Based    â”‚  â”‚ (Trained)â”‚       â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                   â”‚
â”‚  â”‚  â”‚ Hash     â”‚  â”‚ Hybrid   â”‚       â”‚                   â”‚
â”‚  â”‚  â”‚ Based    â”‚  â”‚ (Combined)â”‚       â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Feature Extraction                â”‚                   â”‚
â”‚  â”‚  - UID (64-bit â†’ 8 bytes)           â”‚                   â”‚
â”‚  â”‚  - Frontend digit (1-9)             â”‚                   â”‚
â”‚  â”‚  - Backend number                   â”‚                   â”‚
â”‚  â”‚  - Global ID                        â”‚                   â”‚
â”‚  â”‚  - Text length                      â”‚                   â”‚
â”‚  â”‚  - Character frequencies            â”‚                   â”‚
â”‚  â”‚  - Stream type (one-hot)            â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Embedding Generation              â”‚                   â”‚
â”‚  â”‚  [Feature-based]                    â”‚                   â”‚
â”‚  â”‚    â””â”€ Direct feature â†’ vector      â”‚                   â”‚
â”‚  â”‚  [Semantic]                         â”‚                   â”‚
â”‚  â”‚    â””â”€ Trained model lookup          â”‚                   â”‚
â”‚  â”‚  [Hash-based]                       â”‚                   â”‚
â”‚  â”‚    â””â”€ Hash â†’ normalized vector     â”‚                   â”‚
â”‚  â”‚  [Hybrid]                           â”‚                   â”‚
â”‚  â”‚    â”œâ”€ Text embedding (optional)    â”‚                   â”‚
â”‚  â”‚    â””â”€ Feature embedding            â”‚                   â”‚
â”‚  â”‚    â””â”€ Weighted combination          â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Dimension Projection               â”‚                   â”‚
â”‚  â”‚  - Project to target dimension       â”‚                   â”‚
â”‚  â”‚  - Normalize vector                 â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  Embedding Vector (float32 array)                          â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Embedding Strategies:**

```
Feature-Based Strategy:
    TokenRecord
        â†“
    Extract Features:
        - UID bytes (8 floats)
        - Frontend digit (1 float)
        - Backend number (1 float)
        - Global ID (1 float)
        - Text length (1 float)
        - Stream type (9 floats, one-hot)
        - Character stats (N floats)
        â†“
    Concatenate â†’ Feature vector
        â†“
    Project to embedding_dim (768 default)
        â†“
    Normalize
        â†“
    Embedding

Semantic Strategy:
    TokenRecord
        â†“
    Lookup UID in trained model
        â†“
    Retrieve learned embedding
        â†“
    Embedding

Hash-Based Strategy:
    TokenRecord
        â†“
    Hash text + UID
        â†“
    Convert to vector
        â†“
    Normalize
        â†“
    Embedding

Hybrid Strategy:
    TokenRecord
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Text Embed  â”‚ Feature Emb â”‚
    â”‚ (optional)  â”‚ (always)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚            â”‚
           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â†“
         Weighted Combination
                 â†“
         Embedding
```

---

### 4. Training System Architecture

**Semantic Model Training** - Train custom embeddings on your corpus.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Training System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Training Corpus                                             â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Tokenization Phase                  â”‚                   â”‚
â”‚  â”‚  - TextTokenizer.build()             â”‚                   â”‚
â”‚  â”‚  - Multiple streams                   â”‚                   â”‚
â”‚  â”‚  - TokenRecord creation               â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Vocabulary Building                â”‚                   â”‚
â”‚  â”‚  - Collect unique tokens             â”‚                   â”‚
â”‚  â”‚  - Build token â†’ index mapping       â”‚                   â”‚
â”‚  â”‚  - Calculate frequencies             â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Co-occurrence Matrix               â”‚                   â”‚
â”‚  â”‚  - Build context windows             â”‚                   â”‚
â”‚  â”‚  - Count co-occurrences               â”‚                   â”‚
â”‚  â”‚  - Create sparse matrix               â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Training Loop                      â”‚                   â”‚
â”‚  â”‚  For each epoch:                     â”‚                   â”‚
â”‚  â”‚    - Sample training pairs           â”‚                   â”‚
â”‚  â”‚    - Forward pass                    â”‚                   â”‚
â”‚  â”‚    - Calculate loss                  â”‚                   â”‚
â”‚  â”‚    - Backward pass                    â”‚                   â”‚
â”‚  â”‚    - Update embeddings                â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Model Saving                       â”‚                   â”‚
â”‚  â”‚  - Save embeddings                   â”‚                   â”‚
â”‚  â”‚  - Save vocabulary                   â”‚                   â”‚
â”‚  â”‚  - Save metadata                     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Enhanced Training (Multi-Stream):**

```
Multiple Token Streams
    â”œâ”€ char stream
    â”œâ”€ subword stream
    â””â”€ word stream
        â†“
[Multi-Stream Learning]
    â”œâ”€ Learn at all granularities
    â”œâ”€ Cross-stream alignment
    â””â”€ Hierarchical semantics
    â†“
[Temporal Awareness]
    â”œâ”€ Position-dependent embeddings
    â””â”€ Sequence modeling
    â†“
[Content-ID Clustering]
    â”œâ”€ Deterministic grouping
    â””â”€ Semantic clusters
    â†“
[Mathematical Properties]
    â”œâ”€ Frontend/backend integration
    â””â”€ UID-based relationships
    â†“
Enhanced Embeddings
```

---

### 5. API Server Architecture

**FastAPI Server** - Production-ready RESTful API.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK API Server Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Request Layer                          â”‚    â”‚
â”‚  â”‚  - HTTP Requests (REST)                            â”‚    â”‚
â”‚  â”‚  - WebSocket Connections                            â”‚    â”‚
â”‚  â”‚  - File Uploads                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Middleware Layer                       â”‚    â”‚
â”‚  â”‚  - CORS handling                                    â”‚    â”‚
â”‚  â”‚  - Authentication (JWT)                           â”‚    â”‚
â”‚  â”‚  - Request validation                              â”‚    â”‚
â”‚  â”‚  - Error handling                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Route Handlers                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚ Tokenize â”‚  â”‚ Embed    â”‚  â”‚ Train    â”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚ Upload   â”‚  â”‚ Search   â”‚  â”‚ Jobs     â”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Processing Layer                       â”‚    â”‚
â”‚  â”‚  - TextTokenizer                                    â”‚    â”‚
â”‚  â”‚  - EmbeddingGenerator                               â”‚    â”‚
â”‚  â”‚  - VectorStore                                       â”‚    â”‚
â”‚  â”‚  - JobManager (async)                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Response Layer                          â”‚    â”‚
â”‚  â”‚  - JSON responses                                    â”‚    â”‚
â”‚  â”‚  - Streaming responses                               â”‚    â”‚
â”‚  â”‚  - WebSocket messages                                â”‚    â”‚
â”‚  â”‚  - File downloads                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API Endpoint Structure:**

```
/api/v1/
    â”œâ”€ POST /tokenize
    â”‚   â””â”€ Text â†’ Tokens
    â”œâ”€ POST /embed
    â”‚   â””â”€ Text â†’ Embeddings
    â”œâ”€ POST /train
    â”‚   â””â”€ Corpus â†’ Model
    â”œâ”€ POST /upload
    â”‚   â””â”€ File â†’ Processing
    â”œâ”€ GET /search
    â”‚   â””â”€ Query â†’ Results
    â”œâ”€ GET /jobs/{id}
    â”‚   â””â”€ Job status
    â””â”€ WebSocket /ws
        â””â”€ Real-time streaming
```

---

### 6. Vector Store Architecture

**Vector Database Integration** - Multiple backend support.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SanTOK Vector Store Architecture               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Embeddings + Metadata                                       â”‚
â”‚      â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Vector Store Interface             â”‚                   â”‚
â”‚  â”‚  (Abstract base)                     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Backend Selection                  â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                   â”‚
â”‚  â”‚  â”‚ ChromaDB â”‚  â”‚  FAISS   â”‚         â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                   â”‚
â”‚  â”‚  â”‚ Weaviate â”‚  â”‚ In-Memoryâ”‚         â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                 â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Storage Operations                  â”‚                   â”‚
â”‚  â”‚  - add(embedding, metadata)           â”‚                   â”‚
â”‚  â”‚  - search(query, top_k)               â”‚                   â”‚
â”‚  â”‚  - get(id)                            â”‚                   â”‚
â”‚  â”‚  - delete(id)                         â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 7. Complete Data Flow Architecture

**End-to-End Processing Pipeline:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Complete SanTOK Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Input: Text/String                                          â”‚
â”‚      â†“                                                       â”‚
â”‚  [1] Preprocessing                                           â”‚
â”‚      â”œâ”€ Normalize case                                       â”‚
â”‚      â”œâ”€ Clean punctuation                                    â”‚
â”‚      â””â”€ Detect language                                      â”‚
â”‚      â†“                                                       â”‚
â”‚  [2] Tokenization (9 methods in parallel)                   â”‚
â”‚      â”œâ”€ Space, Word, Char                                    â”‚
â”‚      â”œâ”€ Grammar, Subword                                     â”‚
â”‚      â””â”€ BPE, Syllable, Frequency, Byte                      â”‚
â”‚      â†“                                                       â”‚
â”‚  [3] Mathematical Analysis                                   â”‚
â”‚      â”œâ”€ UID assignment (Xorshift64*)                        â”‚
â”‚      â”œâ”€ Frontend digit (9-centric)                          â”‚
â”‚      â”œâ”€ Backend number                                       â”‚
â”‚      â””â”€ Global ID                                            â”‚
â”‚      â†“                                                       â”‚
â”‚  [4] Embedding Generation                                    â”‚
â”‚      â”œâ”€ Feature extraction                                   â”‚
â”‚      â”œâ”€ Strategy selection                                   â”‚
â”‚      â””â”€ Vector generation                                    â”‚
â”‚      â†“                                                       â”‚
â”‚  [5] Storage/Reasoning (Optional)                            â”‚
â”‚      â”œâ”€ Vector Store (ChromaDB/FAISS/Weaviate)             â”‚
â”‚      â””â”€ Cognitive Reasoning (Knowledge Graph)               â”‚
â”‚      â†“                                                       â”‚
â”‚  Output: Tokens + Embeddings + Metadata                      â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8. Component Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User/API   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server      â”‚
â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚              â”‚
       â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TextTokenizerâ”‚  â”‚ EmbeddingGen â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚         â”‚       â”‚
       â†“         â†“       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Cognitive â”‚ â”‚ Vector   â”‚ â”‚ Training â”‚
â”‚Reasoning â”‚ â”‚ Store    â”‚ â”‚ System   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Design Principles

1. **Modularity**: Each component is independent and can be used separately
2. **Determinism**: Same input always produces same output
3. **Extensibility**: Easy to add new tokenization methods or embedding strategies
4. **Performance**: Parallel processing where possible
5. **Scalability**: Supports large-scale processing
6. **Explainability**: Full traceability of all operations

---

## ğŸš€ Installation

### Prerequisites

- **Python**: 3.11 or higher
- **pip**: Python package installer
- **RAM**: 4GB minimum, 8GB recommended
- **Disk Space**: 2GB free space

### Method 1: Automated Setup (Recommended)

**Linux/Mac:**
```bash
git clone <repository-url>
cd SanTOK-Code-Only
chmod +x setup.sh  # if setup script exists
./setup.sh
```

**Windows:**
```powershell
git clone <repository-url>
cd SanTOK-Code-Only
.\setup.bat  # if setup script exists
```

### Method 2: Manual Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd SanTOK-Code-Only
```

2. **Create virtual environment:**
```bash
python -m venv venv

# Activate virtual environment
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **Install the package (optional):**
```bash
pip install -e .
```

### Method 3: Docker (If Available)

```bash
docker-compose up
```

### Verify Installation

```bash
python check_system.py
```

Or test in Python:
```python
from santok import TextTokenizationEngine

engine = TextTokenizationEngine()
result = engine.tokenize("Hello World", "whitespace")
print(result['tokens'])  # Should print: ['Hello', 'World']
```

---

## âš¡ Quick Start

### Method 1: Basic Tokenization (Python)

```python
from santok import TextTokenizationEngine

# Create engine
engine = TextTokenizationEngine(
    random_seed=12345,
    normalize_case=True,
    remove_punctuation=False
)

# Tokenize text
text = "Hello World! This is SanTOK."
result = engine.tokenize(text, tokenization_method="whitespace")

print(f"Tokens: {result['tokens']}")
print(f"Frontend Digits: {result['frontend_digits']}")
print(f"Features: {result['features']}")
```

### Method 2: Using Core Tokenizer (Advanced)

```python
from src.core.core_tokenizer import TextTokenizer

# Create tokenizer
tokenizer = TextTokenizer(seed=42, embedding_bit=False)

# Build token streams (multiple methods at once)
streams = tokenizer.build("Hello World! This is SanTOK.")

# Access different tokenization methods
word_tokens = streams["word"].tokens
char_tokens = streams["char"].tokens
subword_tokens = streams["subword"].tokens

# Each token has: text, uid, index, content_id, frontend_digit, backend_number, global_id
for token in word_tokens[:5]:
    print(f"Text: {token.text}, UID: {token.uid}, Frontend: {token.frontend_digit}")
```

### Method 3: Using the CLI

```bash
# Tokenize text
python santok_cli.py tokenize --text "Hello world" --method word

# Tokenize file
python santok_cli.py tokenize --file data.txt --output tokens.json

# Train embeddings
python santok_cli.py train --file corpus.txt --model-path model.pkl

# Generate embeddings
python santok_cli.py embed --text "Hello world" --model-path model.pkl

# Show system information
python santok_cli.py info
```

### Method 4: Start the API Server

```bash
# Option 1: Using run script (recommended)
python run.py

# Option 2: Direct start
python start.py

# Option 3: For Railway/Heroku deployment
python main.py  # Auto-detects app from src.servers.main_server

# Server will be available at http://localhost:8000
# Interactive API docs at http://localhost:8000/docs
# Alternative docs at http://localhost:8000/redoc
```

### Method 5: API Example (REST)

```bash
# Tokenize via API
curl -X POST "http://localhost:8000/api/v1/tokenize" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world", "method": "word"}'

# Generate embeddings
curl -X POST "http://localhost:8000/api/v1/embed" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world", "strategy": "feature_based"}'

# Health check
curl http://localhost:8000/api/v1/health
```

### Method 6: WebSocket (Real-time)

```python
import asyncio
import websockets
import json

async def tokenize_websocket():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # Send tokenize request
        await websocket.send(json.dumps({
            "action": "tokenize",
            "text": "Hello world",
            "method": "word"
        }))
        
        # Receive results
        result = await websocket.recv()
        print(json.loads(result))

asyncio.run(tokenize_websocket())
```

---

## ğŸ§© Core Components

### 1. Core Tokenization (`santok/` and `src/core/`)

**Main Classes:**
- `TextTokenizationEngine` - Main tokenization engine
- `TextTokenizer` - Core tokenizer with multiple methods
- `BaseTokenizer` - Base class for custom tokenizers
- `ParallelTokenizer` - Parallel processing support

**Tokenization Methods:**
- `space` / `whitespace` - Split by whitespace characters
- `word` - Word-based tokenization (alphabetic characters)
- `char` / `character` - Character-level tokenization
- `grammar` - Grammar-aware tokenization with punctuation handling
- `subword` - Basic subword tokenization
- `subword_bpe` - Byte-Pair Encoding (BPE) subword tokenization
- `subword_frequency` - Frequency-based subword tokenization
- `subword_syllable` - Syllable-based subword tokenization
- `byte` - Byte-level tokenization (ord-based)

**Multi-language Support:**
- Automatic language detection
- Support for CJK (Chinese, Japanese, Korean)
- Arabic, Cyrillic, Hebrew, Thai, Devanagari support
- Language-specific word boundary detection

### 2. Embeddings (`src/embeddings/`)

**Components:**
- `SanTOKEmbeddingGenerator` - Generate embeddings from text
- `SanTOKVectorStore` - Store and search embeddings
- `SanTOKSemanticTrainer` - Train semantic models
- `SanTOKInferencePipeline` - Inference pipeline
- `EnhancedSanTOKSemanticTrainer` - Enhanced training with multi-stream learning

**Embedding Strategies:**
- `feature_based` - Mathematical feature-based embeddings
- `hash_based` - Hash-based embeddings
- `semantic` - Trained semantic embeddings
- `hybrid` - Combination of multiple strategies

### 3. Vector Stores (`src/embeddings/` and `weaviate_codes/`)

**Supported Databases:**
- **ChromaDB** - Lightweight vector database
- **FAISS** - Facebook AI Similarity Search
- **Weaviate** - Cloud-native vector database

### 4. Cognitive Reasoning (`santok_cognitive/`)

**Components:**
- `UnifiedMemory` - Unified memory system
- `SanTOKReasoner` - Symbolic reasoning engine
- `GraphStore` - Knowledge graph storage
- `TreeStore` - Hierarchical tree storage
- `InferenceEngine` - Inference rule engine

**Features:**
- 15+ relation types
- 20+ inference rules
- Contradiction detection
- Confidence propagation
- Full explainability

### 5. API Servers (`src/servers/`)

**Available Servers:**
- `main_server.py` - Full-featured FastAPI server
- `lightweight_server.py` - Lightweight API server
- `simple_server.py` - Simple HTTP server
- `api_server.py` - Alternative API implementation

**Features:**
- RESTful API endpoints
- WebSocket support
- File upload/download
- Job management
- Authentication (JWT)
- Interactive documentation

### 6. Training (`src/training/` and `enhanced_semantic_trainer/`)

**Components:**
- `SanTOKVocabularyBuilder` - Build vocabularies
- `SanTOKLanguageModelTrainer` - Train language models
- `EnhancedSanTOKSemanticTrainer` - Enhanced semantic training
- `DatasetDownloader` - Download training datasets

### 7. Small Language Models (`santok_cognitive/slm/`)

**Components:**
- Transformer-based small language models
- Training scripts
- Model loading and inference
- Vocabulary expansion

### 8. Integration (`src/integration/`)

**Components:**
- `VocabularyAdapter` - Adapt vocabularies between systems
- `SourceMapIntegration` - Source map integration
- `CognitivePipeline` - Integration with cognitive reasoning

### 9. Utilities (`santok/utils/` and `src/utils/`)

**Components:**
- `Config` - Configuration management
- `LoggingConfig` - Logging setup
- `Validation` - Input validation
- `UniqueIdentifier` - UID generation

---

## ğŸ“– Usage Examples

### Example 1: Comprehensive Text Analysis

```python
from santok import TextTokenizationEngine

engine = TextTokenizationEngine()

# Analyze with all methods
text = "SanTOK is an advanced text processing framework."
analysis = engine.analyze_text(text)

# Access results for each method
for method, result in analysis.items():
    print(f"{method}: {len(result['tokens'])} tokens")
    print(f"  Frontend Digits: {result['frontend_digits']}")
    print(f"  Features: {result['features']}")
```

### Example 2: Semantic Embedding Training

```python
from src.core.core_tokenizer import TextTokenizer
from src.embeddings.semantic_trainer import SanTOKSemanticTrainer

# Tokenize corpus
tokenizer = TextTokenizer(seed=42)
streams = tokenizer.build(your_corpus_text)

# Train semantic embeddings
trainer = SanTOKSemanticTrainer(
    embedding_dim=768,
    epochs=10,
    window_size=5
)

# Collect all tokens
all_tokens = []
for stream in streams.values():
    all_tokens.extend(stream.tokens)

# Build vocabulary and train
trainer.build_vocab(all_tokens)
trainer.build_cooccurrence(all_tokens)
trainer.train(all_tokens)

# Save model
trainer.save("model.pkl")

# Get embedding for a token
embedding = trainer.get_embedding(token_uid)
```

### Example 3: Enhanced Semantic Training

```python
from enhanced_semantic_trainer import EnhancedSanTOKSemanticTrainer
from src.core.core_tokenizer import TextTokenizer

# Tokenize
tokenizer = TextTokenizer()
streams = tokenizer.build(your_text)

# Train with enhanced features
trainer = EnhancedSanTOKSemanticTrainer(
    embedding_dim=768,
    epochs=10,
    window_size=5,
    use_multi_stream=True,
    use_temporal=True,
    use_content_id_clustering=True,
    use_math_properties=True
)

trainer.train(streams)
trainer.save("enhanced_model.pkl")
```

### Example 4: Cognitive Reasoning

```python
from santok_cognitive import UnifiedMemory, SanTOKReasoner

# Create memory
memory = UnifiedMemory()

# Add knowledge
memory.add("Python is a programming language", "fact", auto_link_graph=True)
memory.add("Programming languages are used for software development", "fact", auto_link_graph=True)

# Create reasoner
reasoner = SanTOKReasoner(memory)

# Ask question
answer = reasoner.ask("What is Python?")

print(answer.text)
print(answer.explain())  # Full reasoning trace
```

### Example 5: Vector Store Integration

```python
from src.embeddings.embedding_generator import SanTOKEmbeddingGenerator
from src.embeddings.vector_store import SanTOKVectorStore

# Generate embeddings
generator = SanTOKEmbeddingGenerator(strategy="feature_based")
embedding = generator.generate("Hello world")

# Store in vector database
store = SanTOKVectorStore()
doc_id = store.add(embedding, metadata={"text": "Hello world", "id": 1})

# Search
query_embedding = generator.generate("greeting")
results = store.search(query_embedding, top_k=5)

for result in results:
    print(f"Score: {result['score']}, Metadata: {result['metadata']}")
```

### Example 6: API Server Usage

```python
from fastapi import FastAPI
from santok import TextTokenizationEngine

app = FastAPI()
engine = TextTokenizationEngine()

@app.post("/tokenize")
async def tokenize(text: str, method: str = "whitespace"):
    result = engine.tokenize(text, method)
    return result

# Run with: uvicorn main:app --reload
```

---

## ğŸ“š API Documentation

### REST API Endpoints

When the server is running, visit `http://localhost:8000/docs` for interactive Swagger documentation or `http://localhost:8000/redoc` for ReDoc documentation.

**Core Endpoints:**

- `POST /api/v1/tokenize` - Tokenize text with multiple methods
- `POST /api/v1/embed` - Generate embeddings from text
- `POST /api/v1/train` - Train semantic embedding model
- `GET /api/v1/health` - Health check endpoint
- `GET /api/v1/info` - System information
- `POST /api/v1/analyze` - Comprehensive text analysis

**File Operations:**

- `POST /api/v1/upload` - Upload file for processing
- `POST /api/v1/tokenize/file` - Tokenize uploaded file
- `GET /api/v1/download/{file_id}` - Download processed results

**WebSocket:**

- `WebSocket /ws` - Real-time streaming tokenization
- `WebSocket /ws/train` - Real-time training progress

**Job Management:**

- `POST /api/v1/jobs` - Create async job
- `GET /api/v1/jobs/{job_id}` - Get job status
- `GET /api/v1/jobs/{job_id}/result` - Get job result

**Example Requests:**

**Tokenize Text:**
```bash
curl -X POST "http://localhost:8000/api/v1/tokenize" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello world",
    "method": "word",
    "compute_features": true,
    "seed": 42
  }'
```

**Response:**
```json
{
  "tokens": ["Hello", "world"],
  "frontend_digits": [5, 6],
  "backend_numbers": [123, 456],
  "global_ids": [789, 101],
  "features": {
    "length_factor": 2,
    "balance_index": 5,
    "entropy_index": 0,
    "mean": 5.5,
    "variance": 0.25
  },
  "method": "word",
  "token_count": 2
}
```

**Generate Embeddings:**
```bash
curl -X POST "http://localhost:8000/api/v1/embed" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello world",
    "strategy": "feature_based",
    "model_path": "model.pkl"
  }'
```

**Upload and Process File:**
```bash
curl -X POST "http://localhost:8000/api/v1/upload" \
  -F "file=@document.txt" \
  -F "method=word"
```

**WebSocket Example:**
```python
import asyncio
import websockets
import json

async def tokenize_stream():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # Send request
        await websocket.send(json.dumps({
            "action": "tokenize",
            "text": "Hello world",
            "method": "word"
        }))
        
        # Receive streaming results
        while True:
            result = await websocket.recv()
            data = json.loads(result)
            if data.get("done"):
                break
            print(f"Token: {data.get('token')}")

asyncio.run(tokenize_stream())
```

---

## ğŸ’» CLI Usage

### Tokenization Commands

```bash
# Tokenize text
python santok_cli.py tokenize --text "Hello world" --method word

# Tokenize file
python santok_cli.py tokenize --file data.txt --output tokens.json --format json

# Tokenize from URL
python santok_cli.py tokenize --url https://example.com/text.txt
```

### Training Commands

```bash
# Train basic model
python santok_cli.py train --file corpus.txt --model-path model.pkl

# Train with enhanced trainer
python santok_cli.py train --file corpus.txt --model-path model.pkl --enhanced

# Custom training parameters
python santok_cli.py train --file corpus.txt \
  --model-path model.pkl \
  --embedding-dim 768 \
  --epochs 20 \
  --window-size 5
```

### Embedding Commands

```bash
# Generate embeddings
python santok_cli.py embed --text "Hello world" --model-path model.pkl

# Generate with different strategy
python santok_cli.py embed --text "Hello world" \
  --strategy feature_based \
  --output embeddings.npy
```

### Utility Commands

```bash
# Run tests
python santok_cli.py test

# Quick tests
python santok_cli.py test --quick

# Show system information
python santok_cli.py info
```

### Using the santok CLI (if installed)

```bash
# After installation: pip install -e .
santok "Hello world" --method whitespace
santok "Hello world" --analyze --output results.json
```

---

## ğŸš¢ Deployment

### Local Development

```bash
# Start development server
python run.py

# Or use uvicorn directly
uvicorn src.servers.main_server:app --reload --host 0.0.0.0 --port 8000
```

### Production Deployment

**Using Railway:**
```bash
# Railway auto-detects start.py
# Set PORT environment variable
railway up
```

**Using Docker:**
```bash
docker-compose up -d
```

**Using systemd (Linux):**
```bash
# Create service file
sudo nano /etc/systemd/system/santok.service

# Start service
sudo systemctl start santok
sudo systemctl enable santok
```

### Environment Variables

- `PORT` - Server port (default: 8000)
- `LOG_LEVEL` - Logging level (default: INFO)
- `WEAVIATE_URL` - Weaviate server URL (optional)
- `WEAVIATE_API_KEY` - Weaviate API key (optional)

---

## ğŸ“ Project Structure

```
SanTOK-Code-Only/
â”œâ”€â”€ santok/                      # Core tokenization package
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ santok.py                # Main TextTokenizationEngine class
â”‚   â”œâ”€â”€ cli.py                   # CLI interface (argparse-based)
â”‚   â””â”€â”€ utils/                   # Utility modules
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â”œâ”€â”€ logging_config.py    # Logging setup
â”‚       â””â”€â”€ validation.py        # Input validation
â”‚
â”œâ”€â”€ santok_cognitive/            # Cognitive reasoning system
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md                # Cognitive system documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md          # Architecture documentation
â”‚   â”œâ”€â”€ WHITEPAPER.md            # Technical whitepaper
â”‚   â”œâ”€â”€ graph/                   # Knowledge graph implementation
â”‚   â”‚   â”œâ”€â”€ graph_node.py       # Graph node class
â”‚   â”‚   â”œâ”€â”€ graph_edge.py        # Graph edge class
â”‚   â”‚   â”œâ”€â”€ graph_store.py       # Graph storage
â”‚   â”‚   â””â”€â”€ relation_extractor.py # Relation extraction
â”‚   â”œâ”€â”€ trees/                   # Hierarchical tree structures
â”‚   â”‚   â”œâ”€â”€ tree.py              # Tree implementation
â”‚   â”‚   â”œâ”€â”€ tree_node.py         # Tree node class
â”‚   â”‚   â””â”€â”€ tree_store.py        # Tree storage
â”‚   â”œâ”€â”€ memory/                  # Unified memory system
â”‚   â”‚   â”œâ”€â”€ unified_memory.py   # Main memory class
â”‚   â”‚   â””â”€â”€ memory_object.py    # Memory object representation
â”‚   â”œâ”€â”€ reasoning/               # Inference and reasoning
â”‚   â”‚   â”œâ”€â”€ santok_reasoner.py  # Main reasoner
â”‚   â”‚   â”œâ”€â”€ inference_engine.py # Inference rule engine
â”‚   â”‚   â”œâ”€â”€ query_engine.py      # Query processing
â”‚   â”‚   â”œâ”€â”€ path_finder.py       # Path finding algorithms
â”‚   â”‚   â”œâ”€â”€ contradiction_detector.py # Contradiction detection
â”‚   â”‚   â””â”€â”€ explainer.py        # Explanation generation
â”‚   â”œâ”€â”€ algorithms/              # Custom SanTOK algorithms
â”‚   â”‚   â”œâ”€â”€ santok_ranker.py    # Hybrid relevance ranking
â”‚   â”‚   â”œâ”€â”€ nine_scorer.py      # 9-centric confidence scoring
â”‚   â”‚   â”œâ”€â”€ semantic_similarity.py # Semantic similarity
â”‚   â”‚   â”œâ”€â”€ graph_walker.py     # Graph traversal algorithms
â”‚   â”‚   â””â”€â”€ pattern_matcher.py  # Pattern matching
â”‚   â”œâ”€â”€ slm/                     # Small Language Models
â”‚   â”‚   â”œâ”€â”€ santok_slm_model.py # SLM model implementation
â”‚   â”‚   â”œâ”€â”€ tiny_slm.py          # Tiny transformer model
â”‚   â”‚   â”œâ”€â”€ slm_trainer.py      # Training scripts
â”‚   â”‚   â””â”€â”€ [multiple training scripts]
â”‚   â””â”€â”€ integration/             # Integration modules
â”‚       â”œâ”€â”€ cognitive_pipeline.py # Cognitive processing pipeline
â”‚       â”œâ”€â”€ vector_bridge.py    # Vector store bridge
â”‚       â””â”€â”€ token_bridge.py      # Token bridge
â”‚
â”œâ”€â”€ santok_complete/             # Complete production system
â”‚   â”œâ”€â”€ core/                    # Core tokenization
â”‚   â”œâ”€â”€ embeddings/              # Embedding generation
â”‚   â”œâ”€â”€ training/                # Model training
â”‚   â”œâ”€â”€ servers/                 # API servers
â”‚   â””â”€â”€ vector_stores/           # Vector database integrations
â”‚
â”œâ”€â”€ src/                         # Main source code
â”‚   â”œâ”€â”€ core/                    # Core tokenization engines
â”‚   â”‚   â”œâ”€â”€ core_tokenizer.py   # Main tokenizer (9 methods)
â”‚   â”‚   â”œâ”€â”€ base_tokenizer.py   # Base tokenizer class
â”‚   â”‚   â””â”€â”€ parallel_tokenizer.py # Parallel processing
â”‚   â”œâ”€â”€ embeddings/              # Embedding systems
â”‚   â”‚   â”œâ”€â”€ embedding_generator.py # Embedding generation
â”‚   â”‚   â”œâ”€â”€ semantic_trainer.py  # Semantic model training
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # Vector storage
â”‚   â”‚   â”œâ”€â”€ weaviate_vector_store.py # Weaviate integration
â”‚   â”‚   â””â”€â”€ inference_pipeline.py # Inference pipeline
â”‚   â”œâ”€â”€ servers/                 # API servers
â”‚   â”‚   â”œâ”€â”€ main_server.py      # Full-featured FastAPI server
â”‚   â”‚   â”œâ”€â”€ lightweight_server.py # Lightweight server
â”‚   â”‚   â”œâ”€â”€ simple_server.py    # Simple HTTP server
â”‚   â”‚   â”œâ”€â”€ api_server.py       # Alternative API implementation
â”‚   â”‚   â”œâ”€â”€ job_manager.py       # Async job management
â”‚   â”‚   â””â”€â”€ error_handling.py   # Error handling utilities
â”‚   â”œâ”€â”€ training/                # Training modules
â”‚   â”‚   â”œâ”€â”€ vocabulary_builder.py # Vocabulary construction
â”‚   â”‚   â”œâ”€â”€ language_model_trainer.py # Language model training
â”‚   â”‚   â””â”€â”€ dataset_downloader.py # Dataset management
â”‚   â”œâ”€â”€ integration/             # Integration modules
â”‚   â”‚   â”œâ”€â”€ vocabulary_adapter.py # Vocabulary adaptation
â”‚   â”‚   â””â”€â”€ source_map_integration.py # Source map integration
â”‚   â”œâ”€â”€ compression/             # Compression algorithms
â”‚   â”‚   â””â”€â”€ compression_algorithms.py # Text compression
â”‚   â”œâ”€â”€ interpretation/          # Text interpretation
â”‚   â”‚   â””â”€â”€ data_interpreter.py  # Data interpretation
â”‚   â”œâ”€â”€ performance/             # Performance testing
â”‚   â”‚   â”œâ”€â”€ test_accuracy.py     # Accuracy tests
â”‚   â”‚   â””â”€â”€ comprehensive_performance_test.py # Full benchmarks
â”‚   â”œâ”€â”€ cli/                     # CLI tools
â”‚   â”‚   â””â”€â”€ main.py              # CLI main entry
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â””â”€â”€ unique_identifier.py # UID generation
â”‚
â”œâ”€â”€ backend/                     # Backend-specific code
â”‚   â”œâ”€â”€ santok/                  # Backend tokenization package
â”‚   â”œâ”€â”€ src/                     # Backend source (mirror of src/)
â”‚   â””â”€â”€ Architecture_Docs/       # Architecture documentation
â”‚
â”œâ”€â”€ enhanced_semantic_trainer/   # Enhanced semantic training
â”‚   â”œâ”€â”€ enhanced_trainer.py     # Enhanced trainer implementation
â”‚   â”œâ”€â”€ example_train.py         # Training examples
â”‚   â”œâ”€â”€ example_use.py          # Usage examples
â”‚   â””â”€â”€ examples/                # Additional examples
â”‚
â”œâ”€â”€ examples/                    # Example scripts and demos
â”‚   â”œâ”€â”€ embedding_example.py    # Embedding examples
â”‚   â”œâ”€â”€ vector_store examples   # Vector store usage
â”‚   â”œâ”€â”€ training examples       # Training examples
â”‚   â””â”€â”€ integration examples    # Integration examples
â”‚
â”œâ”€â”€ docs/                        # Comprehensive documentation
â”‚   â”œâ”€â”€ api/                     # API documentation
â”‚   â”œâ”€â”€ backend/                 # Backend documentation
â”‚   â”œâ”€â”€ examples/                # Example documentation
â”‚   â”œâ”€â”€ guides/                   # User guides
â”‚   â”œâ”€â”€ integration/             # Integration guides
â”‚   â””â”€â”€ performance/             # Performance documentation
â”‚
â”œâ”€â”€ weaviate_codes/              # Weaviate integration
â”‚   â”œâ”€â”€ weaviate_vector_store.py # Weaviate vector store
â”‚   â””â”€â”€ README.md                # Weaviate setup guide
â”‚
â”œâ”€â”€ main.py                      # Main entry point (Railway/Heroku)
â”œâ”€â”€ run.py                       # Cross-platform run script
â”œâ”€â”€ start.py                     # Server startup script
â”œâ”€â”€ santok_cli.py                # Main CLI interface
â”œâ”€â”€ check_system.py              # System verification script
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package setup configuration
â”œâ”€â”€ Procfile                     # Heroku/Railway process file
â”œâ”€â”€ runtime.txt                  # Python version specification
â””â”€â”€ README.md                    # This comprehensive documentation
```

---

## ğŸ“š Advanced Examples & Use Cases

### Complete Workflow Examples

SanTOK provides comprehensive example scripts demonstrating various use cases:

#### 1. Basic Tokenization Examples

**File**: `examples/embedding_example.py`
- Basic tokenization and embedding generation
- Token-by-token embedding visualization
- Document-level embeddings
- Vector store integration

**File**: `examples/train_semantic_embeddings.py`
- Training semantic embeddings from scratch
- Vocabulary building
- Model persistence

#### 2. Vector Store Examples

**File**: `examples/comprehensive_vector_store_example.py`
- Unified example combining ALL vector store capabilities
- Weaviate, FAISS, and ChromaDB integration
- Semantic search with filtering
- Concept exploration and clustering
- Context fusion embeddings
- Batch processing for large datasets

**File**: `examples/use_vector_store.py`
- Loading vector stores from disk
- Interactive search mode
- Cluster analysis
- Similarity comparisons

**File**: `examples/search_examples.py`
- Advanced search patterns
- Multi-level concept exploration
- Related concept finding
- Token comparison utilities

#### 3. Large-Scale Processing Examples

**File**: `examples/test_full_workflow_500k.py`
- Complete workflow for 500K+ token datasets
- Batch processing with disk saving
- Resume capability
- Memory-efficient embedding generation
- Wikipedia data integration

#### 4. Cognitive Reasoning Examples

**File**: `santok_cognitive/demo.py`
- Knowledge graph construction
- Tree-based hierarchical organization
- Symbolic reasoning demonstrations
- Inference rule applications
- Full pipeline examples

**File**: `santok_cognitive/showcase.py`
- Advanced cognitive features
- Query answering with explanations
- Contradiction detection
- Confidence propagation

#### 5. Integration Examples

**File**: `examples/integrate_source_map_workflow.py`
- Source map integration
- Metadata tracking
- Railway compute workflows

**File**: `examples/integration_with_transformers.py`
- Integration with external transformer models
- Hybrid embedding strategies
- Model comparison

**File**: `examples/quick_start_integration.py`
- Quick integration guide
- Common integration patterns

#### 6. Small Language Model Examples

**File**: `examples/santok_with_tiny_slm.py`
- SanTOK-native SLM usage
- Constraint-grounded generation
- No external AI dependencies

**File**: `examples/simple_tiny_slm.py`
- Basic SLM implementation
- Training and inference

#### 7. Quality Evaluation Examples

**File**: `examples/eval_embedding_quality.py`
- Embedding quality assessment
- Probe token evaluation
- Semantic alignment testing

**File**: `examples/compare_neighbors.py`
- Comparison between different stores/strategies
- Overlap analysis
- Performance benchmarking

#### 8. Data Interpretation Examples

**File**: `examples/test_data_interpreter.py`
- Real-time data interpretation
- Weaviate-based knowledge discovery
- Semantic relationship extraction

### Use Case Scenarios

#### Scenario 1: Document Processing Pipeline

```python
# 1. Tokenize documents
from src.core.core_tokenizer import TextTokenizer
tokenizer = TextTokenizer(method="word", seed=42)
tokens = tokenizer.tokenize_text("Your document text here...")

# 2. Generate embeddings
from src.embeddings.embedding_generator import SanTOKEmbeddingGenerator
generator = SanTOKEmbeddingGenerator(strategy="hybrid")
embeddings = generator.generate_embeddings(tokens)

# 3. Store in vector database
from src.embeddings.vector_store import ChromaVectorStore
store = ChromaVectorStore(collection_name="documents")
store.add_tokens(tokens, embeddings)

# 4. Semantic search
results = store.search(embeddings[0], top_k=10)
```

#### Scenario 2: Knowledge Base Construction

```python
# 1. Build knowledge graph
from santok_cognitive.memory.unified_memory import UnifiedMemory
memory = UnifiedMemory()

# 2. Add facts
obj1 = memory.add("Python is a programming language", "fact")
obj2 = memory.add("Python uses dynamic typing", "fact")

# 3. Create relationships
memory.add_relation(obj1.uid, obj2.uid, RelationType.RELATED_TO)

# 4. Query with reasoning
from santok_cognitive.reasoning.reasoner import SanTOKReasoner
reasoner = SanTOKReasoner(memory.graph)
answer = reasoner.answer("What is Python?")
print(answer.explanation)
```

#### Scenario 3: Real-Time Text Analysis

```python
# 1. Set up inference pipeline
from src.embeddings.inference_pipeline import SanTOKInferencePipeline
pipeline = SanTOKInferencePipeline(
    embedding_strategy="semantic",
    vector_store="chroma"
)

# 2. Process incoming text
result = pipeline.process_text(
    "Machine learning is a subset of artificial intelligence",
    store=True
)

# 3. Find similar concepts
similar = pipeline.similarity_search(
    "deep learning",
    top_k=5
)
```

#### Scenario 4: Training Custom Models

```python
# 1. Build vocabulary
from src.training.vocabulary_builder import SanTOKVocabularyBuilder
builder = SanTOKVocabularyBuilder()
vocab = builder.build_vocabulary("corpus.txt")

# 2. Train language model
from src.training.language_model_trainer import SanTOKLanguageModelTrainer
trainer = SanTOKLanguageModelTrainer(vocab)
model = trainer.train("corpus.txt", epochs=10)

# 3. Generate text
generated = model.generate("The future of AI", max_length=100)
```

#### Scenario 5: API Integration

```python
# Start API server
python run.py

# Use REST API
import requests

# Tokenize
response = requests.post("http://localhost:8000/api/v1/tokenize", json={
    "text": "Hello world",
    "method": "word"
})
tokens = response.json()

# Generate embeddings
response = requests.post("http://localhost:8000/api/v1/embed", json={
    "text": "Hello world",
    "strategy": "feature_based"
})
embeddings = response.json()
```

### Performance Optimization Examples

**File**: `src/performance/comprehensive_performance_test.py`
- Performance benchmarking
- Tokenizer comparison
- Reconstruction accuracy testing
- Speed optimization strategies

### Running Examples

To run any example:

```bash
# Navigate to examples directory
cd examples

# Run specific example
python comprehensive_vector_store_example.py

# Or run from project root
python examples/embedding_example.py
```

### Example Outputs

Most examples generate:
- **Token files**: JSON/CSV files with tokenized data
- **Embedding files**: NumPy arrays or pickle files
- **Vector store files**: Persistent database files
- **Report files**: Markdown/JSON reports with results
- **Visualization files**: PNG/SVG charts and graphs

---

## ğŸ§ª Testing

### Automated Tests

```bash
# Quick smoke tests via CLI
python santok_cli.py test --quick

# Full test suite (if pytest tests exist)
python -m pytest tests/

# With coverage report
python -m pytest tests/ --cov=santok --cov-report=html

# Test specific module
python -m pytest tests/test_tokenization.py -v
```

### System Verification

```bash
# Check system setup and dependencies
python check_system.py

# This verifies:
# - Python version
# - Installed dependencies
# - File structure
# - Basic functionality
```

### Manual Testing

**Test Tokenization:**
```python
from santok import TextTokenizationEngine

engine = TextTokenizationEngine()
result = engine.tokenize("Hello World", "whitespace")
assert len(result['tokens']) == 2
assert result['tokens'][0] == 'Hello'
print("âœ“ Tokenization test passed")
```

**Test Core Tokenizer:**
```python
from src.core.core_tokenizer import TextTokenizer

tokenizer = TextTokenizer(seed=42)
streams = tokenizer.build("Hello World")
assert "word" in streams
assert len(streams["word"].tokens) > 0
print("âœ“ Core tokenizer test passed")
```

**Test Embeddings:**
```python
from src.embeddings.embedding_generator import SanTOKEmbeddingGenerator

generator = SanTOKEmbeddingGenerator(strategy="feature_based")
embedding = generator.generate("Hello world")
assert embedding is not None
assert len(embedding) > 0
print("âœ“ Embedding generation test passed")
```

**Test API Server:**
```bash
# Start server
python start.py &

# Test health endpoint
curl http://localhost:8000/api/v1/health

# Test tokenize endpoint
curl -X POST "http://localhost:8000/api/v1/tokenize" \
  -H "Content-Type: application/json" \
  -d '{"text": "test", "method": "word"}'
```

### Performance Testing

```python
# Run performance benchmarks
from src.performance.comprehensive_performance_test import run_performance_tests

results = run_performance_tests()
print(results)
```

### Example Test Scripts

Check the `examples/` directory for comprehensive test examples:
- `test_full_workflow_500k.py` - Large-scale workflow test
- `eval_embedding_quality.py` - Embedding quality evaluation
- `test_data_interpreter.py` - Data interpretation tests

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Make your changes**
4. **Run tests**
   ```bash
   python santok_cli.py test
   ```
5. **Commit your changes**
   ```bash
   git commit -m "Add your feature description"
   ```
6. **Push to your fork**
   ```bash
   git push origin feature/your-feature-name
   ```
7. **Submit a Pull Request**

### Contribution Guidelines

- Follow PEP 8 style guidelines
- Add docstrings to all functions and classes
- Include tests for new features
- Update documentation as needed
- Keep commits atomic and well-described

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ‘¤ Author

**Santosh Chavala**

- GitHub: [@chavalasantosh](https://github.com/chavalasantosh)
- Repository: [SanTOK](https://github.com/chavalasantosh/SanTOK)
- DeepWiki: [SanTOK Documentation](https://deepwiki.com/chavalasantosh/SanTOK)

---

## ğŸ™ Acknowledgments

- Built with Python 3.11+
- Uses FastAPI for API servers
- Integrates with Weaviate, ChromaDB, and FAISS
- Thanks to all contributors and the open-source community

---

## ğŸ“Š Project Statistics

- **Total Files**: 300+ Python files
- **Lines of Code**: 50,000+
- **Components**: 15+ major modules
- **Tokenization Methods**: 9+
- **Supported Python Versions**: 3.11+
- **API Endpoints**: 20+
- **Inference Rules**: 20+ (Cognitive)

---

## ğŸ”— Additional Resources

### Documentation Files

- **[SanTOK Cognitive Documentation](santok_cognitive/README.md)** - Cognitive reasoning system
- **[SanTOK Cognitive Architecture](santok_cognitive/ARCHITECTURE.md)** - Detailed architecture
- **[SanTOK Cognitive Whitepaper](santok_cognitive/WHITEPAPER.md)** - Technical whitepaper
- **[SanTOK Complete Documentation](santok_complete/README.md)** - Complete system docs
- **[Enhanced Trainer Documentation](enhanced_semantic_trainer/README.md)** - Enhanced training
- **[Weaviate Integration Guide](weaviate_codes/README.md)** - Weaviate setup and usage

### Interactive Documentation

- **API Swagger Docs**: `http://localhost:8000/docs` (when server is running)
- **API ReDoc**: `http://localhost:8000/redoc` (when server is running)

### Documentation Directories

- `docs/api/` - API documentation
- `docs/backend/Architecture_Docs/` - Backend architecture
- `docs/examples/` - Example documentation
- `docs/guides/` - User guides
- `docs/integration/` - Integration guides
- `docs/performance/` - Performance documentation

### Example Scripts

Check the `examples/` directory for:
- Embedding examples
- Vector store usage
- Training workflows
- Integration examples
- Performance benchmarks

---

## ğŸ†˜ Support & Troubleshooting

### Common Issues

**Port already in use:**
```bash
# Change port
PORT=8001 python run.py
```

**Python version too old:**
- Install Python 3.11+ from [python.org](https://www.python.org/downloads/)

**Dependencies fail to install:**
```bash
pip install --upgrade pip
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

**Import errors:**
- Ensure you're in the project root directory
- Activate virtual environment
- Run `python check_system.py` to diagnose issues

### Getting Help

1. Check the documentation in `docs/`
2. Run `python check_system.py` to verify installation
3. Check server logs for error messages
4. Review examples in `examples/` directory
5. Open an issue on GitHub

---

## ğŸ“– Quick Reference / Cheat Sheet

### Common Operations

**Tokenization:**
```python
from santok import TextTokenizationEngine
engine = TextTokenizationEngine(seed=42)
result = engine.tokenize("Hello World", method="word")
```

**Embeddings:**
```python
from src.embeddings.embedding_generator import SanTOKEmbeddingGenerator
generator = SanTOKEmbeddingGenerator(strategy="feature_based")
embeddings = generator.generate_embeddings(token_records)
```

**Vector Store:**
```python
from src.embeddings.vector_store import ChromaVectorStore
store = ChromaVectorStore(collection_name="docs")
store.add_tokens(tokens, embeddings)
results = store.search(query_embedding, top_k=10)
```

**Cognitive Reasoning:**
```python
from santok_cognitive.memory.unified_memory import UnifiedMemory
memory = UnifiedMemory()
obj = memory.add("Python is a language", "fact")
answer = memory.search_by_content("What is Python?")
```

**API Server:**
```bash
# Start server
python run.py

# Tokenize via API
curl -X POST http://localhost:8000/api/v1/tokenize \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello World", "method": "word"}'
```

### Common Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `seed` | Random seed for reproducibility | `42` |
| `method` | Tokenization method | `"word"` |
| `strategy` | Embedding strategy | `"feature_based"` |
| `embedding_dim` | Embedding dimension | `768` |
| `top_k` | Number of results | `10` |

### File Locations

| Component | Location |
|-----------|----------|
| Core Tokenizer | `src/core/core_tokenizer.py` |
| Embeddings | `src/embeddings/` |
| Vector Stores | `src/embeddings/vector_store.py` |
| Cognitive | `santok_cognitive/` |
| API Server | `src/servers/main_server.py` |
| Examples | `examples/` |
| CLI | `santok_cli.py` |

---

## ğŸ­ Industry Use Cases & Applications

### Healthcare & Medical AI

**Use Case**: Explainable medical diagnosis support
- **Challenge**: Medical AI must be explainable and auditable
- **SanTOK Solution**: 
  - Deterministic reasoning with full explainability
  - Knowledge graphs for medical relationships
  - Constraint enforcement for safety
- **Benefits**: 
  - Traceable decisions
  - Regulatory compliance
  - No hallucination in critical medical information

**Example:**
```python
# Medical knowledge base
memory = UnifiedMemory()
memory.add("Aspirin reduces inflammation", source="medical_literature")
memory.add("Patient has inflammation", source="patient_record")

# Query with explanation
result = memory.query("Should patient take aspirin?")
# Returns: Answer + Full reasoning trace + Confidence + Sources
```

### Finance & Banking

**Use Case**: Auditable financial decision systems
- **Challenge**: Financial decisions must be traceable and compliant
- **SanTOK Solution**:
  - Full audit trails
  - Contradiction detection
  - Source tracking
- **Benefits**:
  - Regulatory compliance
  - Risk management
  - Fraud detection

**Example:**
```python
# Financial rules engine
memory = UnifiedMemory()
memory.add("High risk requires approval", relation=RelationType.RULE)
memory.add("Transaction is high risk", source="risk_engine")

# Automated decision with audit trail
decision = memory.reason("Should transaction be approved?")
# Returns: Decision + Complete audit trail + Rule chain
```

### Legal & Compliance

**Use Case**: Legal document analysis and reasoning
- **Challenge**: Legal reasoning must be precise and explainable
- **SanTOK Solution**:
  - Symbolic reasoning for legal logic
  - Knowledge graphs for case law
  - Full explainability
- **Benefits**:
  - Precise legal analysis
  - Case law relationships
  - Explainable conclusions

**Example:**
```python
# Legal knowledge base
memory = UnifiedMemory()
memory.add("Contract breach requires damages", relation=RelationType.IMPLIES)
memory.add("Party A breached contract", source="evidence")

# Legal reasoning
conclusion = memory.reason("What are the legal consequences?")
# Returns: Conclusion + Legal reasoning chain + Precedents
```

### Enterprise Knowledge Management

**Use Case**: Internal knowledge bases with guarantees
- **Challenge**: Enterprise knowledge must be reliable and searchable
- **SanTOK Solution**:
  - Unified memory (vector + graph + tree)
  - Source tracking
  - Temporal awareness
- **Benefits**:
  - Reliable knowledge retrieval
  - Source attribution
  - Knowledge evolution tracking

**Example:**
```python
# Enterprise knowledge base
memory = UnifiedMemory()
memory.add("Product X uses technology Y", source="engineering_team", date="2024-01-15")
memory.add("Technology Y is deprecated", source="tech_lead", date="2024-03-20")

# Temporal-aware query
results = memory.search("What technology does Product X use?")
# Returns: Current answer + Historical changes + Source timeline
```

### Customer Support

**Use Case**: AI-powered customer support with full audit trails
- **Challenge**: Support responses must be accurate and traceable
- **SanTOK Solution**:
  - Constraint-grounded generation
  - Knowledge validation
  - Full audit trails
- **Benefits**:
  - Accurate responses
  - Source attribution
  - Quality assurance

**Example:**
```python
# Support knowledge base
memory = UnifiedMemory()
memory.add("Feature X requires subscription Y", source="product_docs")
memory.add("Customer has subscription Y", source="customer_db")

# Support query
response = memory.query("Can customer use Feature X?")
# Returns: Answer + Knowledge sources + Confidence + Audit trail
```

### Research & Academia

**Use Case**: Research paper analysis and knowledge extraction
- **Challenge**: Extract and reason about research findings
- **SanTOK Solution**:
  - Semantic embeddings for paper similarity
  - Knowledge graphs for research relationships
  - Citation tracking
- **Benefits**:
  - Research discovery
  - Citation networks
  - Knowledge synthesis

**Example:**
```python
# Research knowledge base
memory = UnifiedMemory()
memory.add("Study A shows X causes Y", source="paper_123", citation=True)
memory.add("Study B contradicts Study A", source="paper_456", citation=True)

# Research query
findings = memory.query("What is the relationship between X and Y?")
# Returns: Findings + Contradictions + Citations + Confidence
```

### Software Development

**Use Case**: Code analysis and documentation
- **Challenge**: Understand code relationships and generate documentation
- **SanTOK Solution**:
  - Code tokenization (supports any file type)
  - Semantic embeddings for code similarity
  - Knowledge graphs for code relationships
- **Benefits**:
  - Code understanding
  - Documentation generation
  - Refactoring support

**Example:**
```python
# Code knowledge base
memory = UnifiedMemory()
memory.add("Function X calls Function Y", source="codebase", relation=RelationType.CALLS)
memory.add("Function Y is deprecated", source="changelog")

# Code analysis
analysis = memory.query("What functions does Function X depend on?")
# Returns: Dependencies + Status + Recommendations
```

---

## â“ Frequently Asked Questions (FAQ)

### General Questions

**Q: What makes SanTOK different from other tokenizers?**
A: SanTOK provides deterministic UIDs, mathematical properties (frontend/backend numbers), perfect reversibility, and integrates tokenization with embeddings, vector stores, and cognitive reasoning - all in one framework.

**Q: Do I need external models (BERT, GPT, etc.) to use SanTOK?**
A: No! SanTOK is self-contained. You can train your own embeddings and models using only SanTOK components. External models are optional for hybrid strategies.

**Q: Is SanTOK production-ready?**
A: Yes! SanTOK includes production-ready APIs, error handling, logging, monitoring, and deployment configurations for platforms like Railway and Heroku.

**Q: What file types does SanTOK support?**
A: SanTOK supports ANY file type - text, images, videos, audio, binary files, executables, archives, and more. It's a universal tokenization system.

**Q: How fast is SanTOK?**
A: SanTOK is optimized for performance with parallel processing, caching, and efficient algorithms. See the Performance Benchmarks section for detailed metrics.

### Technical Questions

**Q: What is a deterministic UID?**
A: A deterministic UID is a unique identifier that is always the same for the same token. Same input = same UID, every time. This enables reproducible results.

**Q: What are frontend and backend numbers?**
A: Frontend digits (1-9) represent semantic categories, while backend numbers provide positional encoding. These mathematical properties help models understand relationships.

**Q: Can I use SanTOK with existing models?**
A: Yes! SanTOK provides adapters and integration tools to work with external models, transformers, and other NLP tools.

**Q: How do I choose between embedding strategies?**
A: 
- **Feature-based**: Fast, deterministic, no training needed
- **Semantic**: Best quality, requires training on your data
- **Hash-based**: Ultra-fast, good for large-scale applications
- **Hybrid**: Combines multiple strategies for best results

**Q: What vector store should I use?**
A:
- **FAISS**: Fast, in-memory, good for development
- **ChromaDB**: Persistent, disk-based, good for local deployment
- **Weaviate**: Cloud-native, scalable, best for production

### Training Questions

**Q: How much data do I need to train embeddings?**
A: Minimum 100K tokens recommended, but 1M+ tokens produces better results. The more domain-specific data, the better.

**Q: How long does training take?**
A: Depends on dataset size and hardware. Typical training: 10-30 minutes for 1M tokens on modern CPUs, faster with GPUs.

**Q: Can I resume training?**
A: Yes! SanTOK supports checkpointing and resuming training from saved models.

**Q: How do I know if my model is ready?**
A: Check the Model Readiness Checklist in the documentation. Key indicators: loss converged, perplexity reasonable, generation quality acceptable.

### Deployment Questions

**Q: Can I deploy SanTOK to cloud platforms?**
A: Yes! SanTOK includes configurations for Railway, Heroku, and other platforms. See the Deployment section for details.

**Q: What are the system requirements?**
A: Python 3.11+, 2GB+ RAM recommended, more for large datasets. No GPU required (but helps for training).

**Q: Is SanTOK secure?**
A: Yes! SanTOK includes JWT authentication, input validation, safe error handling, and security best practices.

---

## ğŸ“Š Comparison with Alternatives

### SanTOK vs. Standard Tokenizers

| Feature | Standard Tokenizers | SanTOK |
|---------|-------------------|--------|
| Deterministic UIDs | âŒ | âœ… |
| Mathematical Properties | âŒ | âœ… |
| Perfect Reversibility | âŒ | âœ… |
| Multiple Granularities | Limited | âœ… (9+ methods) |
| Embedding Integration | âŒ | âœ… |
| Vector Store Integration | âŒ | âœ… |
| Cognitive Reasoning | âŒ | âœ… |
| Self-Contained | âŒ | âœ… |

### SanTOK vs. RAG Systems

| Feature | RAG | SanTOK |
|---------|-----|--------|
| Structured Knowledge | âŒ | âœ… |
| Inference Rules | âŒ | âœ… (20+) |
| Constraint Enforcement | âŒ | âœ… |
| Explainability | âŒ | âœ… Full |
| No Hallucination | âŒ | âœ… |
| Deterministic | âŒ | âœ… |

### SanTOK vs. Knowledge Graphs

| Feature | Knowledge Graphs | SanTOK |
|---------|-----------------|--------|
| Natural Language Output | âŒ | âœ… |
| Inference Rules | Limited | âœ… (20+) |
| Constraint Enforcement | âŒ | âœ… |
| Full Explainability | Partial | âœ… |
| Integration with LLMs | âŒ | âœ… |

### SanTOK vs. Standard Embeddings

| Feature | Standard Embeddings | SanTOK Embeddings |
|---------|-------------------|------------------|
| External Dependencies | âœ… Required | âŒ Optional |
| Domain-Specific | âŒ Generic | âœ… Your domain |
| Mathematical Properties | âŒ | âœ… |
| Training Required | âŒ Pre-trained | âœ… Self-trained |
| Speed | Slow (50ms+) | Fast (2ms) |

---

## âš ï¸ Known Limitations

### Current Limitations

1. **Large Vocabulary Training**: Training embeddings on vocabularies >100K tokens may require significant memory. Use sparse representations or batch processing.

2. **Language Support**: SanTOK works best with English text. Other languages may require additional preprocessing.

3. **GPU Acceleration**: While SanTOK can use GPUs, it's primarily optimized for CPU usage. GPU support is optional.

4. **Real-time Processing**: Very large files (>10GB) may require chunked processing rather than real-time.

5. **Vector Store Scaling**: FAISS and ChromaDB have practical limits. For very large scale (>100M vectors), consider Weaviate.

### Workarounds

- **Large Vocabularies**: Use `max_vocab_size` parameter to limit vocabulary
- **Memory Issues**: Enable batch processing and disk saving
- **Performance**: Use parallel processing for large datasets
- **Scaling**: Use Weaviate for cloud-native, scalable vector storage

---

## ğŸ’¡ Best Practices & Recommendations

### Tokenization

1. **Choose the Right Method**: 
   - Use `word` for general text
   - Use `subword` for code or technical text
   - Use `char` for character-level analysis

2. **Set a Seed**: Always use a consistent seed for reproducible results:
   ```python
   tokenizer = TextTokenizer(seed=42)
   ```

3. **Enable Features**: Compute features for better embeddings:
   ```python
   result = tokenizer.tokenize_text(text, compute_features=True)
   ```

### Embeddings

1. **Train on Your Domain**: Don't rely on generic embeddings - train on your specific domain data.

2. **Use Hybrid Strategy**: For best results, use hybrid embeddings combining feature-based and semantic.

3. **Normalize Embeddings**: Always normalize embeddings for consistent similarity calculations.

4. **Batch Processing**: For large datasets, use batch processing to avoid memory issues.

### Training

1. **Sufficient Data**: Use at least 100K tokens, preferably 1M+ for good results.

2. **Multiple Epochs**: Train for at least 10 epochs, more for complex domains.

3. **Validation**: Always validate on held-out data to prevent overfitting.

4. **Checkpointing**: Save models regularly to enable resuming training.

### Deployment

1. **Use Production Servers**: Use `main_server.py` for production, not `simple_server.py`.

2. **Enable Authentication**: Use JWT authentication for production APIs.

3. **Monitor Performance**: Enable logging and monitoring for production deployments.

4. **Use Vector Stores**: For production, use persistent vector stores (ChromaDB or Weaviate).

### Performance

1. **Parallel Processing**: Enable parallel processing for large datasets.

2. **Caching**: Enable caching for frequently accessed data.

3. **Batch Operations**: Use batch operations for vector stores.

4. **Choose Right Backend**: Use FAISS for speed, Weaviate for scale.

---

## ğŸ—ºï¸ Roadmap & Future Plans

### Short Term (Next Release)

- [ ] Enhanced GPU acceleration support
- [ ] Additional language support (multilingual tokenization)
- [ ] More vector store backends (Pinecone, Qdrant)
- [ ] Improved documentation and tutorials
- [ ] Performance optimizations

### Medium Term (6 Months)

- [ ] Advanced model compression techniques
- [ ] Distributed training support
- [ ] Enhanced API features (GraphQL support)
- [ ] Web UI for model management
- [ ] More inference rules for cognitive reasoning

### Long Term (1 Year)

- [ ] Full multilingual support
- [ ] Advanced model architectures
- [ ] Integration with more LLM providers
- [ ] Enterprise features (SSO, RBAC)
- [ ] Advanced analytics and monitoring

---

## ğŸ“ Version History

### Current Version: 1.0.0

**Features:**
- Complete tokenization system (9+ methods)
- Semantic embedding training
- Vector store integration (ChromaDB, FAISS, Weaviate)
- Cognitive reasoning system
- Production-ready APIs
- Comprehensive documentation

**For detailed changelog, see:** [CHANGELOG.md](CHANGELOG.md) (if available)

---

## ğŸ”„ Migration Guide

### Migrating from Standard Tokenizers

1. **Replace tokenizer calls**:
   ```python
   # Old
   tokens = tokenizer.tokenize(text)
   
   # New
   from santok import TextTokenizationEngine
   engine = TextTokenizationEngine()
   result = engine.tokenize(text, method="word")
   tokens = result['tokens']
   ```

2. **Update to use UIDs**:
   ```python
   # Old: Random IDs
   token_ids = [random_id(t) for t in tokens]
   
   # New: Deterministic UIDs
   token_ids = [token.uid for token in token_records]
   ```

3. **Migrate embeddings**:
   ```python
   # Old: External embeddings
   embeddings = bert_model.encode(tokens)
   
   # New: SanTOK embeddings
   from src.embeddings.embedding_generator import SanTOKEmbeddingGenerator
   generator = SanTOKEmbeddingGenerator(strategy="semantic")
   embeddings = generator.generate_embeddings(token_records)
   ```

### Migrating from RAG Systems

1. **Replace vector store**:
   ```python
   # Old: Generic vector store
   store = VectorStore()
   
   # New: SanTOK vector store
   from src.embeddings.vector_store import ChromaVectorStore
   store = ChromaVectorStore(collection_name="documents")
   ```

2. **Add cognitive reasoning**:
   ```python
   # Old: Simple retrieval
   results = store.search(query)
   
   # New: Cognitive reasoning
   from santok_cognitive.memory.unified_memory import UnifiedMemory
   memory = UnifiedMemory()
   results = memory.search_by_content(query)
   # Includes: Reasoning, validation, explainability
   ```

---

**SanTOK** - Your complete solution for text processing, from tokenization to cognitive reasoning and production deployment.
